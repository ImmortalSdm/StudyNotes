[提供的源代码](https://github.com/amirbar/speech2gesture)

训练：
```
audio_to_multiple_pose_gan.train
```

对验证集中的随机样本进行推断：
```
audio_to_multiple_pose_gan.predict_to_videos
```

对音频样本进行预测：
```
audio_to_multiple_pose_gan.predict_audio
```

---
### 建立poseGAN模型

> 生成对抗网络GAN由生成器generator和鉴别器discriminator组成
，当 discriminator 无法分辨生成的图片和真实图片，这个网络就拟合了。

```
python -m audio_to_multiple_pose_gan.train --gans 1 --name test_run --arch_g audio_to_pose_gans --arch_d D_patchgan --speaker oliver --output_path ../tmp --train_csv ../train.csv
```

##### poseGAN.self变量：
变量|格式|详细
-|-|-
args||配置参数
sess||tensorflow会话
audio_A|[?,?]|音频用于生成虚假姿势
real_pose|[?,64,98]|真实姿势
fake_pose|[?,64,98]|虚假姿势
pose_regloss||在处理姿势和动作的过程中，运动或姿势上的回归损失
G_gan_loss||从全局D训练生成器的损失，由虚假姿势的得分经过一系列包括求平方和在内的计算得来
fake_pose_score||鉴别器给虚假姿势的得分
real_pose_score||鉴别器给真实姿势的得分
is_training||训练模式还是推理模式
D_loss||全局鉴别器D训练中的损失
G_gan_loss||全局鉴别器D训练生成器的损失
G_loss||生成器G训练中的损失，G_loss = pose_regloss + lambda_gan * G_gan_loss
train_D||定义的用于训练鉴别器的优化器
train_G||定义的用于训练生成器的优化器


##### poseGAN.self.args：
参数|值区间|默认值|备注
-|-|-|-
lambda_gan|float|1|multiplier for the GAN loss (versus the regression loss) for the generator. generator_loss = regression_loss + lambda_gan * GAN_loss
lambda_d|float|1|
d_input|motion / pose|motion|鉴别器要鉴别的类型，目前只看到了鉴别动作
reg_loss|motion / pose / both|pose
reg_loss_type|l1 / l2|l1|回归类型
lambda_motion_reg_loss|
gans|integer||必填，不知道什么意思，很重要
name|
checkpoint|
itr_d|
itr_g|
lr_g|float||生成器的优化器的学习速率
lr_d|float||鉴别器的优化器的学习速率
arch_g|audio_to_pose_gans / audio_to_pose|audio_to_pose_gans
arch_d|D_patchgan|D_patchgan
norm|
train_csv|
speaker||发言人
output_path|
config|
output_videos|integer|1
batch_size||32|在一轮训练中，单次训练的数据数量
epochs||300|训练轮数
seq_len|
sample|

##### 步骤：
1. 初始化real_pose的格式为[?,64,98]
2. 初始化audio_A的格式[?,?]
3. 调用audio_to_pose_gans方法，传入audio_A，得到生成的fake_pose格式[?,64,98]
   - 对声音数据调用初始化
   - 音频编码
   - 解码
   - logits
4. 删除总为[0,0]的基本关键点，防止其因为离散问题破GANs，...
5.

---

### 训练poseGAN模型

---
### 用poseGAN模型进行推理











---
