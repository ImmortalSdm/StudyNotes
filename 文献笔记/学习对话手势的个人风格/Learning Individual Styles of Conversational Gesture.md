# 学习对话手势的个人风格
Learning Individual Styles of Conversational Gesture

## 2.相关工作

- 手语和象征性手势识别(Sign language and emblematic gesture recognition)

大量的计算机视觉工作涉及从视频中识别手语手势。
这包括使用视频成绩单作为监督的薄弱来源的方法[3]，以及基于CNN [33，24]和RNN [13]的最新方法。也有一些工作可以识别象征性的手势和面部手势[16、14]，头部手势[31]和同声手势[34]。相比之下，我们的目标是从音频预测同语音手势。

- 对话代理(Conversational agents)

研究人员提出了多种方法来生成合理的手势，特别是对于会话代理的应用[8]。
在早期的工作中，Cassell等人。
[7]提出了一种基于手动定义的规则来指导手臂/手部动作的系统。
随后的基于规则的系统[25]提出了通过注释来表达手势的新方法。
与我们的方法更紧密相关的方法是从语音和文本中学习手势的方法，而无需作者手动指定规则。
值得注意的是，[9]使用口语文本的自然语言处理来合成手势，而Neff [32]提出了一种用于做出特定于人的手势的系统。
莱文等。
[28]学会了使用HMM将声学韵律特征映射到运动。
后来的工作[27]将这种方法扩展为使用强化学习和语音识别，结合文本的声学分析[29]，创建基于规则的混合混合系统[36]以及使用受限的Boltzmann机器进行推理[11]。
由于这些方法的目标是为虚拟代理生成动作，因此它们使用实验室录制的音频，文本和动作捕获。这使他们能够使用简化的假设，这些假设为像我们这样的野生视频分析提供了挑战：例如，[28]需要精确的3D姿势，并假设运动发生在音节边界上，[11]则假设手势由手腕向上运动启动。与这些方法相比，我们的方法在训练过程中没有明确使用任何文本或语言信息-它从原始的视听对应关系中学习手势-也没有使用手定义的手势类别：手臂/手的姿势是直接从音频中预测的。

- 可视化预测手势(Visualizing predicted gestures)

可视化手势的最常见方式之一就是使用它们来为3D化身动画[40、27、19]。由于我们的研究工作是针对无法获得3D数据的野生视频的个性化手势，因此我们使用了Bregler等人启发的数据驱动的综合方法。[2]。为此，我们使用Chan等人的“从视频到视频的姿势”方法。[10]，它使用条件生成对抗网络（GAN）从姿势合成人体视频。

- 声音和视觉(Sound and vision)

Aytar等。
[1]利用自然现象中视觉和音频信号的同步，通过传递来自视觉领域经过训练的判别模型的知识，从未标记的野生视频中学习声音表示。
音频和视频功能的同步也可以用于合成。
Langlois等。
[26]尝试通过生成物体掉落或跌落的刚体动画来优化此类同步事件，这些动画在时间上将输入的声波与所需的接触事件序列与地面进行匹配。
最近，Shlizerman等人。
[37]根据输入的信息对3D化身的手进行了动画处理。
但是，他们的重点是音乐演奏，而不是手势，因此，可能的动作空间有限（例如，小提琴弓的曲折运动）。
此外，虽然音乐是由产生音乐的动作（与音乐同步）唯一定义的，但手势既不是语音所独有的，也不是与语音发声同步的。
在给定音频输入的情况下，一些作品专注于合成说话人的视频的特定任务。
Chung等。
[12]通过学习面部和音频的联合嵌入，从说话者的静止图像和输入的语音片段中生成说话的面部图像。
同样，[39]通过使用递归神经网络将语音音频映射到嘴巴形状，然后将合成的嘴唇嵌入地面真人面部视频中，来合成奥巴马说新颖单词的视频。
虽然这两种方法都可以通过生成说不同人的话的面孔来创建伪造内容，但我们专注于针对动画相同说话者言语而优化的单人模型。
最重要的是，由于手势与语音是异步的，多模式的且针对特定的人，因此从语音生成手势而不是嘴唇动作更为复杂。




## 3.特定发言者的手势数据集






## 5.实验

### 5.1设定

#### 基准线
我们将我们的方法与其他几种模型进行比较。

##### 总是预测中位姿势
演讲者大部分时间都处于休息位置[22]，因此预测演讲者的中位姿势可以作为高质量的基线。有关每个扬声器的静止位置的外观，请参见图2。

##### 预测随机选择的手势
在此基准中，我们从同一说话者的训练集中随机选择一个不同的手势序列（与输入话语不对应），并将其用作我们的预测。尽管我们不希望这种方法在量化上表现良好，但是有理由认为它会产生质感诱人的动作：这是真实的说话人手势，要告诉他们是假的，唯一的方法就是评估其对应的效果。音频。

##### 最近邻
代替从同一说话者选择完全随机的手势序列，我们可以将音频用作相似提示。对于输入音频轨道，我们使用预训练的音频功能找到与扬声器最近的邻居，并传递其相应的运动。为了表示音频，我们使用在AudioSet [17]上预先训练的最新VGGish功能嵌入[20]，并在归一化功能上使用余弦距离。

##### RNN
我们进一步将运动预测与Shlizerman等人提出的RNN架构进行比较。与我们类似，Shlizerman等。从2D骨骼关键点空间中的音频预测手臂和手部动作。但是，尽管我们的模型是带有对数-梅尔频谱图输入的卷积神经网络，但他们的模型使用的是1层LSTM模型，该模型采用MFCC特征（低维，手工制作的音频特征表示）作为输入。我们评估了这两种特征类型，发现对于[37]，MFCC特征在所有说话者上的表现均优于对数mel频谱图特征。因此，我们在实验中使用它们的原始MFCC功能。为了与我们自己的模型保持一致，我们没有像在PCA要素上那样测量L2距离，而是添加了额外的隐藏层并使用L1距离。

##### 我们的，没有GAN
最后，作为消融，我们将完整模型与单独的翻译体系结构的预测进行了比较，而没有对抗性鉴别器。

#### 评估指标
我们主要的量化评估指标是比较不同模型的L1回归损失。我们另外根据正确关键点（PCK）的百分比报告结果[42]，这是一种广泛接受的姿势检测指标。在此，如果预测的关键点落在地面真实关键点的αmax（h，w）个像素之内，则定义为正确，其中h和w分别是人员边界框的高度和宽度。我们将PCK值看得一清二楚，因为它并不是针对部分可见说话者的手势预测而设计的。首先，与L1不同，PCK不是线性的，并且正确性评分在硬阈值之外降至零。由于我们的目标不是预测地面真实运动，而是将其用作训练信号，因此L1更适合衡量我们的平均表现。其次，PCK对较大的手势动作敏感，因为正确半径取决于讲话者手臂的跨度。第三，对人的边界框的依赖引入了人为的敏感性，即每个人在框架中显示了多少，以及他们是坐着还是站着。虽然[42]建议对于有完整人员的数据，α= 0.1，对于只有一半人可见的数据，α= 0.2，但我们对α= 0.1，0.2取平均值，并在补充资料中显示完整结果。

### 5.2 定量评估
我们使用定量指标将方法的结果与基线进行比较。为了评估我们的结果是否令人信​​服，我们进行了一项用户研究。
最后，我们询问所预测的手势是否是特定于人的，并且输入语音是否确实比手势的初始姿势更好地预测运动



### 5.3 定性结果
我们定性地将语音与手势翻译结果与图5中的基线和地面真相手势序列进行比较。请参考我们的补充视频结果，以更好地传达时间信息

## 结论
人类通过视觉和声音进行交流，但是这些方式之间的联系仍然不清楚[22]。在本文中，我们提出了从“狂野”语音中预测特定于人的手势的任务，作为研究这些沟通渠道之间联系的一种计算手段。我们创建了一个针对特定人群的视频数据集，并使用该数据集训练了一个模型来预测语音手势。在实验评估中，我们的模型优于其他方法。
尽管在这些任务上表现出色，但我们的模型仍存在局限性，可以通过结合其他工作来解决。例如，使用音频作为输入有其好处，因为音频是一种丰富的表示形式，其中包含有关韵律，音高，语调，音色，节奏，音调等信息。然而，音频不直接编码高级语言的语义可以允许我们预测某些类型的手势（例如metaphorics）的，也不说话人的语音从其它声音（例如AU dience笑声）分离。其次，我们将姿势估计视为基本事实，这会引入大量噪声，尤其是在说话者的手指上。
我们认为我们的工作是朝对话手势进行计算分析的一步，并为进一步研究打开了三个可能的方向。
第一个是使用手势作为视频分析的表示形式：同语音手势和手臂动作自然成为视频预测任务的目标。
第二种是使用野外手势作为训练对话代理的方法：我们基于GAN [10]提出了一种可视化手势预测的可视化方法，但遵循经典工作[8]，这些方法 预测还可以用于驱动虚拟代理的运动。
最后，我们的方法是从音频预测运动的少数少数尝试之一。
跨模式翻译任务是进一步研究的沃土













---
