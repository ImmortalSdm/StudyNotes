##### tf.placeholder




---
##### tf.layers.batch_normalization
[tf.layers.batch_normalization](https://devdocs.io/tensorflow~python/tf/layers/batch_normalization)

批处理标准化层的功能接口。

“批量标准化：通过减少内部协变量偏移来加速深度网络训练”

---
##### tf.contrib.layers.instance_norm
[tf.contrib.layers.instance_norm](https://devdocs.io/tensorflow~python/tf/contrib/layers/instance_norm)

实例规范化层的功能接口。

“实例规范化：快速风格化的缺失成分”

---
##### tf.transpose
[tf.transpose]()

---
##### tf.gather(<张量>,<索引数组>)

用一个一维的索引数组，将张量中对应索引的向量提取出来

---
##### tf.contrib.signal.stft

---
##### tf.abs




---
##### tf.contrib.signal.linear_to_mel_weight_matrix


---
##### tf.tensordot


---
##### tf.log

---
##### tf.expand_dims

---
##### tf.glorot_uniform_initializer()
[tf.glorot_uniform_initializer](https://devdocs.io/tensorflow~python/tf/glorot_uniform_initializer)

Glorot统一初始化器，也称为Xavier统一初始化器。它从[-limit，limit]内的均匀分布中抽取样本，其中限制为sqrt（6 /（fan_in + fan_out）），其中fan_in是权重张量中的输入单位数，而fan_out是权重中的输出单位数 张量。

---
##### tf.zeros_initializer()
[tf.zeros_initializer](https://devdocs.io/tensorflow~python/tf/zeros_initializer)

---
##### tf.nn.leaky_relu

[tf.nn.leaky_relu](https://devdocs.io/tensorflow~python/tf/nn/leaky_relu)

参数|意义及作用
-|-
features|代表预激活值的张量。 必须为以下类型之一：float16，float32，float64，int32，int64。
alpha|激活函数在x <0处的斜率。
name|操作的名称（可选）。


---
#####

---
#####
