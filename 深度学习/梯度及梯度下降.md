#### 梯度
由全部变量的偏导数组成的向量称为梯度(gradient)
![img](../imgs/4a7bb712-9f3c-460b-8240-721a2e3afba4.png)

梯度指示的方向是各点处函数值变化最大的方向
```
# 求梯度
def numerical_gradient(f,x):
    h=1e-4 # 0.0001
    grad=np.zeros_like(x) # 生成形状和x相同的0数组
    for idx in range(x.size):
        tmp_val=x[idx]

        # f(x+h)
        x[idx]=tmp_val+h
        fxh1=f(x)

        # f(x-h)
        x[idx] = tmp_val - h
        fxh2 = f(x)

        grad[idx]=(fxh1-fxh2)/(2*h)
        x[idx]=tmp_val

    return grad
```

#### 梯度下降法
通过不断地沿着梯度方向前进，逐渐减少函数值的过程就是梯度法。
![img](../imgs/1b402ea6-12dc-4cbc-9ff8-e0eb78a3cefe.png)

![img](../imgs/b3624245-9be0-42fc-9248-0f4446c61b83.png)表示更新量，在神经网络中称为学习率，决定一次学习中学习多少及在多大程度上更新参数。

```
# 梯度下降
def gradient_descent(f,init_x,lr=0.01,step_num=100):
    x=init_x
    for i in range(step_num):
        grad=numerical_gradient(f,x)
        x=x-lr*grad

    return x
```
