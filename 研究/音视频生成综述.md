## 4 Audio and Visual Generation

先前介绍的检索任务表明，训练后的模型能够找到最相似的音频或视觉对应物。
尽管人们可以想象与声音相对应的场景，反之亦然，但多年来研究人员一直试图赋予机器这种想象力。 随着发明和对抗性网络（GAN）的发展[70]，图像或视频生成已成为一个话题。 它涉及几个子任务，包括从潜在空间生成图像或视频[71]，跨模态生成[72、73]等。这些应用程序还与其他任务相关，例如域自适应[74、75]。  。 由于音频和视觉模态之间存在差异，因此机器很难发现它们之间的潜在相关性。 因此，从视觉信号生成声音或反之亦然成为一项艰巨的任务。
在本节中，我们将主要回顾音频和视觉生成的最新发展，即从视觉信号生成音频，反之亦然。 此处的视觉信号主要是指图像，运动动态和视频。  “从视觉到音频”小节主要侧重于从嘴唇区域的视频中恢复语音（图6（a））或生成在给定场景中可能发生的声音（图6（a））。 相比之下，“音频到视觉”生成（图6（b））的讨论将检查从给定音频（图7（a））生成图像，身体运动生成（图7（b））和 说话人脸的产生（图7（c））。

### 4.1 Vision-to-Audio Generation

已经探索了许多从视觉信息中提取音频信息的方法，包括根据视觉观察到的振动预测声音并通过视频信号生成音频。
我们将视音频生成任务分为两类：从口形视频生成语音和从不受场景限制的普通视频合成声音。

#### 4.1.1 Lip Sequence to Speech

言语与嘴唇之间存在自然的关系。 除了通过观察嘴唇（听嘴唇）来理解语音内容之外，一些研究还尝试通过观察嘴唇来重建语音。  Cornu等。  [76]试图从视觉特征中预测频谱包络，将其与人工激发信号相结合，并在语音产生模型中合成音频信号。  Ephrat等。  [82]提出了一种基于CNN的端到端模型，以基于每个静默视频帧的相邻帧为每个静默视频帧生成音频特征。 因此，基于学习到的特征重建波形以产生可理解的语音。
已经广泛地探索了使用时间信息来改善语音重建。  Ephrat等。  [77]提出杠杆老化光流以同时捕获时间运动。  Cornu等。  [78]利用循环神经网络将公司的时间信息纳入预测。

#### 4.1.2 General Video to Audio

当声音撞击某些小物体的表面时，后者会轻微振动。 因此，戴维斯等。  [79]利用这一特殊功能，从高速摄像机被动观察到的振动中恢复声音。 请注意，应使合适的物体容易振动，例如一杯水，一盆植物或一盒餐巾纸。 我们认为这项工作与先前介绍的语音重建研究[76、82、77、78]相似，因为它们都使用了视觉和声音环境之间的关系。 在语音重建中，视觉部分更多地集中在嘴唇运动上，而在这项工作中，它专注于微小的振动。

Owens等。  [80]观察到，当敲打或刮擦不同的材料时，它们会发出各种声音。 因此，研究人员引入了一个模型，该模型学会了从视频中合成声音，在视频中，用不同角度和速度的鼓槌敲打不同材料制成的物体。研究人员证明，他们的模型不仅可以识别源自不同材料的不同声音，而且可以学习与物体互动的方式（对物体施加不同的动作会产生不同的声音）。 该模型利用RNN从视频帧中提取声音特征，并随后通过基于实例的合成过程从生成的波形中提取声音。

虽然欧文斯等。  [80]可能会从各种材料中产生声音，但作者的方法仍然无法应用于现实生活中，因为该网络是在严格的约束下由在实验室环境中拍摄的视频来训练的。 为了改善效果并从野生视频中产生声音，Zhou等人（2003年）提出。  [81]设计了一个端到端模型。 它被构造为视频编码器和声音生成器，以学习从视频帧到声音的映射。 之后，网络利用分层的RNN [83]生成声音。 具体来说，作者训练了一个模型来直接预测来自输入视频的原始音频信号（波形样本）。 他们证明了该模型可以了解各种场景和对象交互的声音和视觉输入之间的关联。

我们前面提到的工作主要集中在单声道音频的产生上，而Morgado等人则是。  [8]尝试将360°摄像机录制的单声道音频转换为空间音频。 执行音频专业化这样的任务需要解决两个主要问题：源分离和本地化。 因此，研究人员设计了一个模型，从混合输入的音频中分离出声源，然后将其定位在视频中。 由于音频和视频是互补的，因此使用了另一个多模态模型来指导分离和本地化。

### 4.2 Audio to Vision

在本节中，我们将详细介绍视听生成。 我们首先介绍音频到图像的生成，它比视频生成更容易，因为它不需要生成的图像之间的时间一致性。

#### 4.2.1 Audio to Image

为了生成质量更好的图像，Wan等人。  [84]提出了一个模型，该模型结合了光谱范数，辅助分类器和投影鉴别器，形成了研究人员的条件GAN模型。 即使是相同的声音，该模型也可以根据声音的音量输出不同比例的图像。
Qiu等人并未产生已经发生的声音的真实世界场景，而是将其显示在屏幕上。  [85]建议从音乐中想象内容。
作者通过将音乐和图像输入两个网络并学习这些功能之间的相关性来提取特征，并最终从所学习的相关性中生成图像。
一些研究集中在视听共同产生上。  Chen等。  [72]是第一个尝试使用条件GAN来解决这个跨模态生成问题的人。 研究人员定义了分别生成图像和声音的声音图像（S2I）网络和图像声音（I2S）网络。  Hao等人没有分离S2I和I2S代。
[86]通过考虑跨模式的循环音频对抗网络（CMCGAN），将各个网络组合成一个网络。 遵循循环一致性原理，CMC GAN由四个子网组成：视听，视听音频，视听音频和视听。
最近，一些研究试图从语音剪辑中重建面部图像。  Duarte等。  [87]通过GAN模型合成了包含表情和姿势的面部图像。
此外，作者通过搜索最佳输入音频长度来提高模型的生成质量。 为了更好地从语音中学习标准化的面部，Oh等人。  [88]探索了一个重建模型。 研究人员通过学习将语音的特征空间与预训练的面部编码器和解码器对齐来训练音频编码器。

图7：演示会说话的脸和移动的身体。

#### 4.2.2 Body Motion Generation

代替直接产生视频，许多研究试图使用动作来给化身动画。 运动合成方法利用了多种技术，例如降维[103、104]，隐马尔可夫模型[105]，高斯过程[106]和神经网络[107、108、109]。
Alemi等。  [89]提出了一种基于条件受限玻尔兹曼机器和递归神经网络的实时GrooveNet，以从音乐中产生舞蹈动作。  Lee等。  [90]利用自回归编码器-解码器网络从音乐中产生一个舞蹈编排系统。  Shlizerman等。  [91]进一步介绍了一个使用“目标延迟” LSTM来预测身体标志的模型。 后者进一步用作产生身体动力学的物质。 关键思想是从音频中创建动画，类似于钢琴家或小提琴手的动作。 总而言之，整个过程生成了与输入音频相对应的艺术家表演视频。
尽管以前的方法可以产生身体运动的动力，但尚未使用音乐的固有节拍信息。 唐等。  [92]提出了一种面向音乐的舞蹈舞蹈合成方法，该方法通过LSTM自动编码器模型提取了声音和运动特征之间的关系。此外，为了获得更好的性能，研究人员使用了掩蔽方法和时间索引来改进他们的模型。 雅尔塔等人提供了薄弱的监督。  [93]探索了从运动方向为运动音乐对齐产生弱标签。 作者通过有条件自动配置的深度RNN（由音频频谱提供）生成了长舞蹈序列。

#### 4.2.3 Talking Face Generation

在探索音频到视频的生成过程中，许多研究人员对从语音或音乐合成人脸产生了极大的兴趣。
这具有许多应用程序，例如动画电影，电话会议，谈话代理和增强语音理解能力，同时又可以保护隐私。 早期关于说话脸生成的研究主要是基于任意语音的音频从数据集中合成特定身份。  Kumar等。  [94]尝试通过使用延时LSTM [110]生成与音频同步的关键点，然后通过另一个网络生成以关键点为条件的视频帧。 此外，Supasorn等。
[95]提出了一种“牙齿替代品”来改善牙齿生成过程中的视觉质量。
随后，钟等人。  [96]尝试使用编码器解码器CNN模型来学习原始音频和视频之间的对应关系。 结合RNN和GAN [70]，Jalalifar等。
[97]产生了一系列逼真的面部，这些面部通过两个网络与输入音频同步。 一种是LSTM网络，用于从音频输入中创建唇形标志。 另一个是有条件的GAN（cGAN），用于生成以给定的嘴唇界标为条件的结果脸。 代替应用cGAN，[98]提出使用时间GAN [111]来提高合成质量。 但是，上述方法仅适用于合成身份仅限于数据集中的说话人脸。

近来，具有任意身份的有声面孔的合成引起了极大的关注。  Chen等。  [99]考虑到语音和嘴唇运动之间的相关性，同时生成多个嘴唇图像。 研究人员利用光流更好地表达了框架之间的信息。 所馈送的光流不仅表示当前形状的信息，还表示先前的时间信息。
正面照片通常在身份和言语上都具有身份。 假设，周等。  [100]使用对抗性学习方法在生成过程中解开一张图像的不同类型信息。 解开的表示形式具有方便的属性，即音频和视频都可以用作生成过程中语音信息的来源。 结果，不仅可以输出特征，而且在应用所得网络时可以更明确地表达它们。
最近，Zhu等人发现音频和视频之间的高级关联。  [73]提出了一种互信息近似，以近似模态之间的互信息。  Chen等。  [101]将地标和运动注意力应用于产生说话的面孔。 作者还提出了一种动态的像素损失，以实现时间一致性。 面部生成不限于诸如音频或视觉的特定模态，因为关键点在于这些不同模态之间是否存在共同的模式。 威尔斯等。  [102]提出了一个名为X2Face的自我监督框架，以学习嵌入的特征并生成目标面部运动。 只要了解嵌入式功能，它就可以从任何输入中产生视频。

表4：近期视听生成研究的摘要

<table>
    <tr><th>类别</th>
        <th>方法</th>
        <th>理念与优势</th>
        <th>缺陷</th></tr>
    <tr><td rowspan="4">音频到图像</td>
        <td></td>
        <td>结合许多现有技术形成GAN</td>
        <td>质量差</td></tr>
    <tr><td></td>
        <td>生成与音乐相关的图像</td>
        <td>质量差</td></tr>
    <tr><td></td>
        <td>生成声音到图像和图像到声音的模型</td>
        <td>模型是独立的</td></tr>
    <tr><td></td>
        <td>提出了一种跨模式循环生成对抗网络</td>
        <td>仅生成图像</td></tr>
    <tr><td rowspan="5">音频到动作</td>
        <td></td>
        <td>结合许多现有技术形成GAN</td>
        <td rowspan="5">限于给定的数据集</td></tr>
    <tr><td></td>
        <td></td></tr>
    <tr><td></td>
        <td></td></tr>
    <tr><td></td>
        <td></td></tr>
    <tr><td></td>
        <td></td></tr>
    <tr><td rowspan="9">Talking Face</td>
        <td></td>
        <td>通过延时LSTM生成关键点</td>
        <td>需要对另一个identity进行再训练</td></tr>
    <tr><td></td>
        <td></td>
        <td></td></tr>
    <tr><td></td>
        <td></td>
        <td></td></tr>
    <tr><td></td>
        <td></td>
        <td></td></tr>
    <tr><td></td>
        <td></td>
        <td></td></tr>
    <tr><td></td>
        <td></td>
        <td></td></tr>
    <tr><td></td>
        <td></td>
        <td></td></tr>
    <tr><td></td>
        <td></td>
        <td></td></tr>
    <tr><td></td>
        <td></td>
        <td></td></tr>
</table>









