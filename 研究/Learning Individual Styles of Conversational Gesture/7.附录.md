### 7.1 数据集
#### 数据的收集与整理
我们通过查询每个发言人的YouTube来收集互联网视频，并使用[16]的方法重复数据。 然后，我们使用了开箱即用的面部识别和姿势检测系统，将每个视频分割为多个间隔，在间隔中，只有被摄对象出现在帧中，并且可以看到所有检测到的关键点。 我们的数据集由60,000个此类间隔组成，平均长度为8.7秒，标准偏差为11.3秒。 总共有144小时的视频。 我们将数据分成80％的训练，10％的验证和10％的测试集，以使每个源视频仅出现在一组中。

#### 数据集注释的质量
我们估计伪地面真相的准确性是否足以通过以下实验支持我们的定量结论。 我们采用了200帧伪真实进行训练，并用3个带有颈部和手臂关键点的人类观察员对其进行了标记。 我们通过σ[i]（每个关键点类型i的标准偏差）来量化注释者之间的共识，这在COCO [31]评估中很典型。 我们还计算了||op[i]-µ[i]||，OpenPose检测与注释均值之间的距离以及||预测-µ[i]||。 音频运动预测和注释平均值之间的距离。 我们发现伪地面真相接近人类标签，因为0.14 = E [|| op[i] - µ[i] ||]≈E [σ[i]] = 0.06; 而且伪地面事实的错误率对于我们的任务来说足够小，因为0.25 = ||预测 - µ[i] ||  >>σ[i]= 0.06。 注意，这是预测误差的下限，因为它是根据训练数据样本计算的。


### 7.2 学习个人手势词典

#### 手势单元分割
我们使用无人监督的方法来构建个人词典。 我们将运动序列分割成手势单元，提出合适的描述符和相似性度量，然后对一个人的手势进行聚类。
手势单元是一系列手势，这些手势从静止位置开始，仅在最后一个手势[23]之后才返回到静止位置。 尽管[34]观察到大多数对象通常一次执行一个手势，但是一项针对电视讲话者的18分钟视频数据集的研究报告说，他们的手势通常会按顺序串在一起[25]。 我们将每个手势单元（从静止位置到静止位置）都视为一个原子片段。

我们使用基于预测误差的无监督方法对手势单元进行时间分割（相比之下，[32]使用监督方法）。 给定从时间t0到tT的关键点的运动序列（第3节），我们尝试预测tT +1姿势。 较低的预测误差可能表示说话者处于静止状态，或者他们处于模型在训练过程中经常看到的手势中间。
由于说话者大部分时间都在静止位置[23]，因此较高的预测误差可能表示新手势已开始。 我们在高预测误差的点上分割手势单元（无需定义每个人的静止位置）。 图6显示了一个分段手势单元的示例。我们训练每个对象的分段模型，并且不希望其在说话者中泛化。

![img](图6.png)
##### 一个手势单元片段

#### 字典学习

我们将在所有静态帧上计算出的关键点的前5个主要成分用作手势单元描述符。 这样可以减少尺寸，同时捕获93％的方差。 我们使用动态时间规整[41]作为我们的距离度量标准，以解决类似手势执行过程中的时间变化。 由于这不是欧几里得范数，因此我们必须计算每对数据点之间的距离。 我们为1,000个训练结构单位的随机选择样本预先计算了距离矩阵，并使用它来对数据点进行分层聚类。

#### 个性化手势

这些群集代表单个人执行的典型手势的无监督定义。 对于每个字典元素群集，我们将中心点定义为平均年龄最接近群集中所有数据点的点。 我们根据每个聚类中的手势单元到中心点的距离对其进行排序，并选择最中心的手势单元进行显示。 我们将图7中为Jon Stewart学习的手势字典的一些示例可视化。