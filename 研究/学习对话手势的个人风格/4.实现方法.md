## 实现方法
　　给定原始的说话语音，我们的目标是生成演讲者的相应手臂和手势动作。我们分两个阶段来处理此任务：

　　1、由于我们训练的唯一信号是相应的音频和姿势检测序列，因此我们使用L1回归到2D关键点时间堆栈来学习从语音到手势的映射。

　　2、为了避免回归所有可能的姿势模型，我们采用了对抗判别器，以确保我们产生的动作是合理的（相对于说话者的典型动作）。

---

### 1、语音到手势翻译
　　任何现实的手势动作必须在时间上连贯且流畅。我们通过学习表示整个语音的音频编码，考虑输入**语音 s** 的整个时间范围并一次（而不是周期性地）预测相应**姿势 p** 的整个时间序列，来实现平滑。

![img](语音到手势转换模型.png)

###### 　　图3：语音到手势转换模型。卷积音频编码器对2D频谱图进行下采样并将其转换为1D信号。然后，转换模型 **G** 预测2D姿势的相应时态堆栈。L1回归到地面真相姿势可提供训练信号，而对抗性判别器 **D** 可确保预测的运动在时间上是连贯的，并且具有说话者的风格。

　　如图所示，整个卷积网络，由一个音频编码器和紧跟着一个1D UNet转换结构组成。

##### 音频：
　　音频编码器采用**2D对数-梅尔频谱图**作为输入，并通过一系列卷积对其进行下采样，从而产生与视频采样率相同的1D信号(15 Hz)。

##### 1D UNet转换结构：
　　UNet转换结构随后通过L1回归损失学会将该信号映射到手势向量的时间堆栈。（有关手势表示的详细信息，请参见第3节）：

![img](5365f664-45dd-4e0a-b470-d0898087bb9d.png)

　　之所以使用UNet架构进行翻译，是因为它的瓶颈为网络提供了过去和未来的时间上下文，允许高频时间信息流过，从而能够预测快速的手势运动。

---

### 2、预测合理的运动

　　虽然L1回归是从数据中提取训练信号的唯一方法，但它存在回归均值的已知问题，这种回归均值会产生过度平滑的运动。为了解决这个问题，添加了一个以预测的姿态序列的差异为条件的**对抗性鉴别器 D**。

　　即，鉴别器的输入是向量m = [p2-p1,......,pT-pT-1]，其中pi是2D姿势关键点，**T** 是输入音频和预测姿势序列的时间范围。鉴别符 **D** 试图使随后的目标最大化，而生成器 **G** （翻译体系结构，第4.1节）则试图使其最小化。

![img](a305a88b-c1e6-437f-b107-133d2bd092fc.png)

　　其中 **s** 是输入音频语音片段，**m** 是预测的姿势堆栈的运动导数。因此，发生器学习产生说话者的真实运动，而鉴别器学习对给定运动序列是否真实进行分类。因此，我们的全部目标是

![img](e3006a03-a293-4eee-a47a-3eb5b098c5d0.png)

---

## 实施细节
　　我们通过从伪地面真实姿势表示中的所有其他关键点减去（每帧）颈部关键点位置来获得平移不变性（第3节）。然后，我们通过减去每个发言人的均值并除以标准差，对所有帧中的每个关键点（例如左手腕）进行标准化。

　　在训练过程中，我们将对应于约4秒钟音频的频谱图作为输入，并预测64个姿态矢量，这对应于15Hz帧率下的4.266667秒，约4秒。

　　在测试时，我们可以运行网络在任意时长的音频内。

　　我们使用Adam[(Adam：随机优化方法)]进行了优化(https://arxiv.org/pdf/1412.6980.pdf)，batch size大小为32，learning rate为0.0001。

　　我们在有对抗损失和没有对抗损失上，分别迭代训练300K和90K次，并选择在验证集上性能最好的模型。
