## 摘要

本文对最近的音视频生成发展进行了全面的调查。将音视频生成分为四个不同的方向：通过发言合成脸部动作视频、通过音频生成肢体动作、声音重建人脸以及音频生成图像。并进一步讨论了每个方向的最新方法以及剩余的挑战。最后，我们总结了常用的数据集和性能指标。

## 1 引言




## 2 通过发言合成脸部动作视频

早期，Chung等[1]，尝试使用编码-解码（encoder-decoder）结构的卷积神经网络（Convolutional Neural Networks，CNN）模型，直接学习原始音频与视频之间的对应关系。

而Kumar等[2]，提出了ObamaNet，一种将文本作为输入并生成相应语音以及同步口型视频的体系结构。

随后，Zhu等[3]，引入了互信息近似（mutual information approximation，MIA）来描述视频与语音之间的连贯性，从而改善了对抗学习中的重构和推理能力。

Jalalifar等[4]，通过结合反馈型神经网络（recurrent neural network）以及条件生成对抗网络（conditional generative adversarial networks，C-GANs），产生更加精确自然的面部视频。

Vougioukas等[5]，提出使用时空生成对抗网络（Temporal generative adversarial networks）进行音频驱动的面部视频的端到端模型。

Wiles等[6]，提出了一个自我监督框架X2Face，使用另一个脸部来驱动脸部的生成。框架不对输入图像的姿势，表情或身份做出任何假设，因此对于那些不受约束的设置会更加鲁棒。

近来，Zhou等[7]，提出了一种分离式的音频-视觉系统（Disentangled Audio-Visual System，DAVS），通过对抗性学习来分离人的身份和言语信息，以更好地生成发言的面部视频。

Chen等[8]，使用一种级联GAN方法，避免了与语音内容无关的视听信号之间的虚假相关性。 还提出一种注意力机制的动态像素损失，避免像素抖动问题并使网络专注于与视听相关的区域。

Mittal[9]等，提出了一种分离音频表示学习框架，这是从音频表示学习的角度提高生成发言面部视频性能的方法。

Gu[10]等，提出了一种双流网络，从多个源图像（而不是单个图像）生成更多的面部细节。

Thies等[11]，提出了一种通用的音频驱动的面部动画方法，可以将目标人员的视频与任何其他人员的音频进行合成发言视频。

R等[12]，设计了一种通道，自动将一个人的说话脸部视频翻译成另外一种语言。

Yi等[13]，引入了个性化的头部姿势以及嘴唇同步，使生成的视频更加逼真。

Cudeiro等[14]，构造了VOCA，一种语音驱动的面部3D动画框架。


Wen等[15]，提出了不相交映射网络（DIMNet），这个模型没有显式地学习模态之间的联合关系，而是将它们分别映射到它们的共同协变量来学习不同模态的共享表示。


## 3 通过音频生成肢体动作

许多研究试图学习声音与肢体动作之间的关联，从而用声音生成肢体动作。Shlizerman等[16]，建立了一个长短期记忆（LSTM）网络，以学习音频特征与人体骨骼标志之间的相关性。 而[17]针对面向音乐的舞蹈合成，开发了LSTM自动编码器模型。Ginosar等人提出了一种时空跨模态翻译的方法[18]，用于学习个人的发言手势风格。Lee等使用一种自回归的编码-解码网络[19]，为给定的音乐输入生成匹配的舞蹈编排。还有研究人员提出了姿势感知损失[20]，模型可以在无标签数据集上进行训练。Deng等[21]利用结构化音频空间的优势，在样式转换问题中采用样本外数据投影的方法。Yalta等[22]提出了一种弱监督深度递归神经网络优化技术，用于舞蹈生成任务。




## 4 声音重建人脸

一个人的声音与他的面部结构、性别、年龄以及种族在统计学上都具有相关性[23]。因此，很多研究人员都致力于通过一个人的声音去合成他的脸部外观。其中，Wen等[23]，使用基于生成对抗网络（GAN）的简单而有效的计算框架。而Duarte等[24]，提出了一种条件GAN，它能够直接从原始语音信号（我们称为Wav2Pix）生成面部图像。[25]介绍了一种用于二进制和多路交叉模式人脸和音频匹配的CNN架构。也有研究人员设计了一种自监督的神经网络模型[26]。


## 5 音频生成图像


[27]中提出了一种自动生成与音乐数据所关联图像的方法。 

等[28]，提出了一种基于属性的音乐到图像翻译的深度学习方法，适用于根据歌曲表达的情感来合成图像。

在这个问题上，Chen等[29]是最早尝试使用条件GAN来实现跨模态生成的研究人员。

为了生成质量更好的图像，Wan等[30]提出了一个结合频谱范数（spectral norm），辅助分类器和投影鉴别器的模型，即使是相同的声音，该模型也可以根据声音的音量的不同进行学习。

Hao等[31]，提出了一种跨模态循环生成对抗网络（CMC-GAN），以处理跨模态音频和视频相互生成。CMCGAN由四种子网循环架构：视-音子网，音-视子网，视-视子网和音-音子网，可以有效地解决视觉和音频模态上的尺寸和结构不对称问题。



## 6 数据集





## 7 讨论

### 7.1 挑战

### 7.2 未来研究方向




[1] J. S. Chung, A. Jamaludin, and A. Zisserman, “You said that?” CoRR, 2017.

[2] R. Kumar, J. Sotelo, K. Kumar, A. de Brebisson, and ´
Y. Bengio, “Obamanet: Photo-realistic lip-sync from text,”
arXiv preprint arXiv:1801.01442, 2017.

[3] H. Zhu, A. Zheng, H. Huang, and R. He, “High-resolution talking face generation via mutual information approximation,” arXiv preprint arXiv:1812.06589, 2018.

[4] S. A. Jalalifar, H. Hasani, and H. Aghajan, “Speech-driven facial reenactment using conditional generative adversarial networks,” CoRR, 2018.

[5] K. Vougioukas, S. Petridis, and M. Pantic, “End-to-end speech-driven facial animation with temporal gans,” in BMVC, 2018.

[6] O. Wiles, A. Koepke, and A. Zisserman, “X2face: A network for controlling face generation by using images, audio, and pose codes,” in European Conference on Computer Vision, 2018.

[7] H. Zhou, Y. Liu, Z. Liu, P. Luo, and X. Wang, “Talking face generation by adversarially disentangled audio-visual representation,” CoRR, 2018.

[8] Z. D. C. X. Lele Chen, Ross K Maddox, “Hierarchical cross-modal talking face generation with dynamic pixelwise loss,” in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.

[9] Gaurav Mittal, Baoyuan Wang. Animating Face using Disentangled Audio Representations. WACV, 2020.

[10] Kuangxiao Gu, Yuqian Zhou, Thomas Huang. FLNet: Landmark Driven Fetching and Learning Network for Faithful Talking Facial Animation Synthesis. AAAI, 2020.

[11] Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, Matthias Nießner. Neural Voice Puppetry: Audio-driven Facial Reenactment. 

[12] Prajwal K R, Rudrabha Mukhopadhyay, Jerin Philip, Abhishek Jha, Vinay Namboodiri, C.V. Jawahar. Towards Automatic Face-to-Face Translation. ACM MM, 2019.

[13] Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, Yong-Jin Liu. Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose. CVPR, 2020.

[14] Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, Michael J. Black. Capture, Learning, and Synthesis of 3D Speaking Styles. CVPR, 2019.

[15]Yandong Wen, Mahmoud Al Ismail, Weiyang Liu, Bhiksha Raj, Rita Singh. Disjoint Mapping Network for Cross-modal Matching of Voices and Faces. International Conference on Learning Representations. ICLR, 2019

[16] E. Shlizerman, L. Dery, H. Schoen, and I. KemelmacherShlizerman, “Audio to body dynamics,” in Proc. CVPR, 2018.

[17] T. Tang, J. Jia, and H. Mao, “Dance with melody: An lstm-autoencoder approach to music-oriented dance synthesis,” in 2018 ACM Multimedia Conference on Multimedia Conference, MM 2018, Seoul, Republic of Korea, October 2226, 2018, 2018, pp. 1598–1606.

[18] Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, Jitendra Malik. Learning Individual Styles of Conversational Gesture. CVPR 2019.

[19] J. Lee, S. Kim, and K. Lee, “Listen to dance: Music-driven choreography generation using autoregressive encoder-decoder network,” CoRR, 2018.

[20]Xuanchi Ren, Haoran Li, Zijian Huang, Qifeng Chen. Music-oriented Dance Video Synthesis with Pose Perceptual Loss 

[21] Kangle Deng, Aayush Bansal, Deva Ramanan. Unsupervised Any-to-Many Audiovisual Synthesis via Exemplar Autoencoders. 

[22] N. Yalta, S. Watanabe, K. Nakadai, and T. Ogata, “Weakly supervised deep recurrent neural networks for basic dance step generation,” CoRR, 2018.

[23] Yandong Wen, Rita Singh, and Bhiksha Raj. Face Reconstruction from Voice using Generative Adversarial Networks. Conference on Neural Information Processing Systems (NeurIPS) 2019

[24] A. Duarte, F. Roldan, M. Tubau, J. Escur, S. Pascual, A. Salvador, E. Mohedano, K. McGuinness, J. Torres, and X. Giro-i Nieto, “Speech-conditioned face generation using generative adversarial networks,” 2019.

[25] Arsha Nagrani, Samuel Albanie, Andrew Zisserman. Seeing Voices and Hearing Faces: Cross-modal Biometric Matching, CVPR 2018.

[26] T.-H. Oh, T. Dekel, C. Kim, I. Mosseri, W. T. Freeman, M. Rubinstein, and W. Matusik, “Speech2face: Learning the face behind a voice,” arXiv preprint arXiv:1905.09773, 2019.

[27] Y. Qiu and H. Kataoka, “Image generation associated with music data,” in 2018 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2018, Salt Lake City, UT, USA, June 18-22, 2018, 2018, pp.
2510–2513.

[28] Nikolaos Passalis, Stavros Doropoulos. DeepSing: Generating Sentiment-aware Visual Stories using Cross-modal Music Translation. arXiv preprint, 2019.

[29] L. Chen, S. Srivastava, Z. Duan, and C. Xu, “Deep cross-modal audio-visual generation,” in Proceedings of the on Thematic Workshops of ACM Multimedia 2017, 2017, pp.
349–357.

[30] C.-H. Wan, S.-P. Chuang, and H.-Y. Lee, “Towards audio to scene image synthesis using generative adversarial network,” arXiv preprint arXiv:1808.04108, 2018.

[31] W. Hao, Z. Zhang, and H. Guan, “Cmcgan: A uniform framework for cross-modal visual-audio mutual generation,” arXiv preprint arXiv:1711.08102, 2017.


---