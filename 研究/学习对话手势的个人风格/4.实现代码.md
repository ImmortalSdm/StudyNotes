[提供的源代码](https://github.com/amirbar/speech2gesture)

提取文件：

```
data.train_test_data_extraction.extract_data_for_training
```

训练：

```
audio_to_multiple_pose_gan.train
```

```
python -m audio_to_multiple_pose_gan.train --gans 1 --name test_run --arch_g audio_to_pose_gans --arch_d D_patchgan --speaker chemistry --output_path ../tmp --train_csv ../train.csv --epochs 1
```



对验证集中的随机样本进行推断：

```
audio_to_multiple_pose_gan.predict_to_videos
```

```
python -m audio_to_multiple_pose_gan.predict_to_videos --train_csv  ../train.csv --seq_len 64 --output_path ../my_output_folder/ --checkpoint ../tmp/audio_to_pose/test_run/2019-12-02--16-12-02-118883/best_ckpt-step_600_validation_loss_1.407.ckp --speaker chemistry -ag audio_to_pose_gans --gans 1
```


对音频样本进行预测：

```
audio_to_multiple_pose_gan.predict_audio
```

```
python -m audio_to_multiple_pose_gan.predict_audio --audio_path ../chemistry/train/audio/67398-00:00:32.432432-00:00:37.103770.wav --output_path ../my_output_folder --checkpoint ../tmp/audio_to_pose/test_run/2019-12-02--16-12-02-118883/best_ckpt-step_600_validation_loss_1.407.ckp --speaker chemistry -ag audio_to_pose_gans --gans 1
```

Crop intervals containing frontal speaker（不知道什么意思）：

> If you just want to use intervals containing frontal view of the speakers for any purpose, follow these instructions

```
data.download.crop_intervals
```

---
### 激活环境
```
source activate speech2gesture_env_py27
```

### 提取文件

```
python -m data.train_test_data_extraction.extract_data_for_training --base_dataset_path ../ --speaker chemistry
```

##### 读取frames_df_10_19_19.csv：
列|含义
-|-
frame_fn|框架文件的名称
pose_dt|视频中的帧时间
pose_fn|包含姿势的序列化文件
speaker|数据集中发言人的姓名
video_fn|视频文件名
old_interval_id|
interval_id|帧出现的时间间隔的唯一ID。 此范围中的间隔是视频的一部分，说话者以清晰正面的方式出现
dataset|train/dev/test
video_link|youtube视频链接

##### train.csv
列|含义
-|-
audio_fn|与训练样本关联的音频文件名的路径
dataset|train/dev/test
start|视频开始时间
end|视频结束时间
pose_fn|包含训练样本的.npz文件的路径
speaker|发言者名
video_fn|视频文件吗




##### 数据缺少情况：
发言者|tgz|video
-|-|-
oliver|✔|✔
jon|✔|✔
conan|✔|✔
rock|✔|✘
chemistry|✔|✔
ellen|✘|✘
almaram|✔|✔
angelica|✔|✔
seth|✔|✘
shelly|✘|✘



---
### 建立poseGAN模型

> 生成对抗网络GAN由生成器generator和鉴别器discriminator组成
，当 discriminator 无法分辨生成的图片和真实图片，这个网络就拟合了。



##### poseGAN.self变量：
变量|格式|详细
-|-|-
args||配置参数
sess||tensorflow会话
audio_A|[?,?]|音频用于生成虚假姿势
real_pose|[?,64,98]|真实姿势
fake_pose|[?,64,98]|虚假姿势
pose_regloss||在处理姿势和动作的过程中，运动或姿势上的回归损失
G_gan_loss||从全局D训练生成器的损失，由虚假姿势的得分经过一系列包括求平方和在内的计算得来
fake_pose_score||鉴别器给虚假姿势的得分
real_pose_score||鉴别器给真实姿势的得分
is_training||训练模式还是推理模式
D_loss||鉴别器D训练中的损失
G_gan_loss||全局鉴别器D训练生成器的损失
G_loss||生成器G训练中的损失，G_loss = pose_regloss + lambda_gan * G_gan_loss
train_D||定义的用于训练鉴别器的优化器
train_G||定义的用于训练生成器的优化器


##### poseGAN.self.args：
参数|参数类型|默认值|备注
-|-|-|-
lambda_gan|float|1|multiplier for the GAN loss (versus the regression loss) for the generator. generator_loss = regression_loss + lambda_gan * GAN_loss
lambda_d|float|1|
d_input|motion / pose|motion|鉴别器要鉴别的类型，目前只看到了鉴别动作
reg_loss|motion / pose / both|pose
reg_loss_type|l1 / l2|l1|回归类型
lambda_motion_reg_loss|
gans|integer||必填，不知道什么意思，很重要
name|
checkpoint|
itr_d|
itr_g|
lr_g|float||生成器的优化器的学习速率
lr_d|float||鉴别器的优化器的学习速率
arch_g|audio_to_pose_gans / audio_to_pose|audio_to_pose_gans|生成器函数
arch_d|D_patchgan|D_patchgan|判别器函数
norm|
train_csv|
speaker||发言人
output_path|
config|audio_to_pose / audio_to_pose_inference|audio_to_pose|处理类型
output_videos|integer|1
batch_size||32|在一轮训练中，单次训练的数据数量
epochs||300|训练轮数
seq_len|
sample|



---

### 训练poseGAN模型


##### 优化器
优化器|详细
-|-
train_D|tf.train.AdamOptimizer( learning_rate=args.lr_d) .minimize(loss=D_loss, var_list=tf.trainable_variables( scope=D_SCOPE))
train_G|tf.train.AdamOptimizer( learning_rate=args.lr_g) .minimize( loss=G_loss, var_list=trainable_variables)

##### 损失函数
用来刻画当前预测值和真实值之间的差距
loss|详细
-|-
G_loss|生成器的损失函数，G_loss = pose_regloss + args.lambda_gan * G_gan_loss
pose_regloss|姿势回归的损失函数
G_gan_loss|生成器的GAN的损失函数
D_loss|判别器的损失函数



---
### 用poseGAN模型进行推理











---
