# 学习对话手势的个人风格
Learning Individual Styles of Conversational Gesture

## 相关工作

- 手语和象征性手势识别(Sign language and emblematic gesture recognition)

大量的计算机视觉工作涉及从视频中识别手语手势。
这包括使用视频成绩单作为监督的薄弱来源的方法[3]，以及基于CNN [33，24]和RNN [13]的最新方法。也有一些工作可以识别象征性的手势和面部手势[16、14]，头部手势[31]和同声手势[34]。相比之下，我们的目标是从音频预测同语音手势。

- 对话代理(Conversational agents)

研究人员提出了多种方法来生成合理的手势，特别是对于会话代理的应用[8]。
在早期的工作中，Cassell等人。
[7]提出了一种基于手动定义的规则来指导手臂/手部动作的系统。
随后的基于规则的系统[25]提出了通过注释来表达手势的新方法。
与我们的方法更紧密相关的方法是从语音和文本中学习手势的方法，而无需作者手动指定规则。
值得注意的是，[9]使用口语文本的自然语言处理来合成手势，而Neff [32]提出了一种用于做出特定于人的手势的系统。
莱文等。
[28]学会了使用HMM将声学韵律特征映射到运动。
后来的工作[27]将这种方法扩展为使用强化学习和语音识别，结合文本的声学分析[29]，创建基于规则的混合混合系统[36]以及使用受限的Boltzmann机器进行推理[11]。
由于这些方法的目标是为虚拟代理生成动作，因此它们使用实验室录制的音频，文本和动作捕获。这使他们能够使用简化的假设，这些假设为像我们这样的野生视频分析提供了挑战：例如，[28]需要精确的3D姿势，并假设运动发生在音节边界上，[11]则假设手势由手腕向上运动启动。与这些方法相比，我们的方法在训练过程中没有明确使用任何文本或语言信息-它从原始的视听对应关系中学习手势-也没有使用手定义的手势类别：手臂/手的姿势是直接从音频中预测的。

- 可视化预测手势(Visualizing predicted gestures)

可视化手势的最常见方式之一就是使用它们来为3D化身动画[40、27、19]。由于我们的研究工作是针对无法获得3D数据的野生视频的个性化手势，因此我们使用了Bregler等人启发的数据驱动的综合方法。[2]。为此，我们使用Chan等人的“从视频到视频的姿势”方法。[10]，它使用条件生成对抗网络（GAN）从姿势合成人体视频。

- 声音和视觉(Sound and vision)

Aytar等。
[1]利用自然现象中视觉和音频信号的同步，通过传递来自视觉领域经过训练的判别模型的知识，从未标记的野生视频中学习声音表示。
音频和视频功能的同步也可以用于合成。
Langlois等。
[26]尝试通过生成物体掉落或跌落的刚体动画来优化此类同步事件，这些动画在时间上将输入的声波与所需的接触事件序列与地面进行匹配。
最近，Shlizerman等人。
[37]根据输入的信息对3D化身的手进行了动画处理。
但是，他们的重点是音乐演奏，而不是手势，因此，可能的动作空间有限（例如，小提琴弓的曲折运动）。
此外，虽然音乐是由产生音乐的动作（与音乐同步）唯一定义的，但手势既不是语音所独有的，也不是与语音发声同步的。
在给定音频输入的情况下，一些作品专注于合成说话人的视频的特定任务。
Chung等。
[12]通过学习面部和音频的联合嵌入，从说话者的静止图像和输入的语音片段中生成说话的面部图像。
同样，[39]通过使用递归神经网络将语音音频映射到嘴巴形状，然后将合成的嘴唇嵌入地面真人面部视频中，来合成奥巴马说新颖单词的视频。
虽然这两种方法都可以通过生成说不同人的话的面孔来创建伪造内容，但我们专注于针对动画相同说话者言语而优化的单人模型。
最重要的是，由于手势与语音是异步的，多模式的且针对特定的人，因此从语音生成手势而不是嘴唇动作更为复杂。




## 数据集

## 方法

给定原始语音语音，我们的目标是生成扬声器的相应手臂和手势动作。我们分两个阶段来处理此任务-首先，由于我们训练的唯一信号是相应的音频和姿势检测序列，因此我们使用L1回归到2D关键点的时间堆栈来学习从语音到手势的映射。其次，为了避免回归所有可能的手势模式，我们采用了对抗判别器，以确保我们产生的动作相对于说话者的典型动作是合理的。

#### 语音到手势翻译

任何现实的手势动作必须在时间上连贯且流畅。我们通过学习表示整个语音的音频编码，考虑输入语音s的整个时间范围并一次（而不是周期性地）预测相应姿势p的整个时间序列，来实现平滑。
  我们的全卷积网络由一个音频编码器和一个1D UNet [35，21]翻译体系结构组成，如图3所示。音频编码器将2D log-mel频谱图作为输入，并通过一个一系列卷积，产生一维信号，其采样率与我们的视频（15 Hz）相同。然后，UNet转换架构通过L1回归损失来学习将该信号映射到姿势向量的时间堆栈（有关手势表示的详细信息，请参见第3节）：

公式

#### 预测合理的运动

虽然L1回归关键点是我们可以从数据中提取训练信号的唯一方法，但它受到已知的均值回归问题的困扰，均值会产生过于平滑的运动，如我们的补充视频结果所示。为了解决这个问题并确保我们产生真实的运动，我们以预测的姿势序列的差异为条件，添加了一个对抗性鉴别器[21，10]D。即，鉴别器的输入是向量m = [p2 p1，...。。。，pT pT T1]，其中pi是2D姿势关键点，T是输入音频和预测姿势序列的时间范围。
鉴别符D试图使随后的目标最大化，而生成器G（翻译体系结构，第4.1节）则试图使其最小化。

公式

其中s是输入音频语音片段，m是预测的姿势堆栈的运动导数。因此，发生器学习产生说话者的真实运动，而鉴别器学习对给定运动序列是否真实进行分类。因此，我们的全部目标是

公式













---
