## 摘要

　　人们普遍同意，成功地训练深度网络需要数千个带注释的训练样本。在本文中，我们提出了一种网络和训练策略，该策略依靠大量使用数据增强功能来更有效地使用可用的带注释的样本。该体系结构包括捕获上下文的收缩路径和实现精确定位的对称扩展路径。我们展示了这样的网络可以从很少的图像进行端到端训练，并且在ISBI挑战方面优于现有的最佳方法（滑动窗口卷积网络），可用于分割电子显微镜堆栈中的神经元结构。使用在透射光显微镜图像（相差和DIC）上训练过的同一网络，我们在这些类别中赢得了2015年ISBI细胞跟踪挑战赛的冠军。而且，网络速度很快。在最新的GPU上，对512x512图像进行分割所需的时间不到一秒钟。

完整的实现（基于Caffe）和受过培训的网络可在http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net上获得。

---


## 引言

　　在过去的两年中，深度卷积网络在许多视觉识别任务中的表现超越了现有技术。 [7,3]。 尽管卷积网络已经存在很长时间了[8]，但由于可用训练集的大小和所考虑的网络的大小，卷积网络的成功受到限制。 Krizhevsky等人的突破。 [7]是由于对具有8层的大型网络和具有100万个训练图像的ImageNet数据集上的数百万个参数进行了监督训练。 从那时起，甚至更大更深的网络也得到了培训[12]。