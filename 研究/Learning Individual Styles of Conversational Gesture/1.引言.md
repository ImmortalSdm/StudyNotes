人类的言语通常伴随着手势和手臂手势。 给定音频语音输入，我们会生成与声音一致的可行手势。 具体来说，我们执行跨模式翻译，从单个发言人的“狂野”单声道语音到他们的手和手臂动作。 我们在未标记的视频上进行训练，对于这些视频，我们仅从自动姿态检测系统获得了嘈杂的伪地面真相。 在定量比较中，我们提出的模型明显优于基线方法。 为了支持对手势和语音之间关系的计算理解的研究，我们发布了一个大型视频集，其中包含特定于人的手势。

当我们交谈时，我们通过两个平行的交流渠道（语音和手势）传达想法。 这些对话性或同声手势是我们说话时自发发出的手和手臂动作[34]。 他们完善语音并添加非语言信息，以帮助听众理解我们所说的内容[6]。  Kendon [23]在一个连续体的一端放置对话手势，在另一端放置手语，一种真正的语言。 在这两种极端之间是哑剧和像“意大利人”这样的标志，带有一致认可的词汇和特定于文化的含义。 可以将手势细分为多个阶段，以描述其从说话者的静止位置到手势准备，笔触，握住和后退到静止状态的过程。

语音和手势传达的信息是否相关？ 这是正在进行辩论的话题。 齐头并进的假设认为，当说话者指的是场景中的主体和物体时，手势对语音是多余的[43]。 相反，根据权衡假设，语音和手势是互补的，因为人们在说话时使用手势需要更多的努力，反之亦然[15]。 我们从数据驱动的学习角度解决这个问题，并问我们可以在多大程度上根据语音的原始音频信号来预测手势运动。

我们提出了一种用于时间跨模态翻译的方法。 给定一个语音陈述的输入音频片段（图1底部），尽管我们从未见过或听到此人说过这样的话，但我们会产生与讲话者风格相匹配的讲话者手臂和手的相应运动。 话语训练（图1中）。
然后，我们使用现有的视频合成方法来可视化说话者说这些话时的样子（图1顶部）。

为了从语音中产生运动，我们必须学习音频和姿势之间的映射。 尽管可以将其公式化为翻译，但实际上，在这种情况下使用自然的视听数据配对存在两个固有的挑战。 首先，手势和语音是异步的，因为手势可以出现在相应的发声之前，之后或之中[4]。 其次，这是一个多模式预测任务，因为说话者可能会在不同场合说同一件事的同时执行不同手势。 而且，获取大量视频的人工注释是可行的。 因此，我们需要从未编码视频上的2D人体姿势检测的伪地面真相中获取训练信号。

尽管如此，我们仍然能够以端到端的方式将语音从原始音频转换为手势到一系列姿势。 为了克服异步性问题，我们使用了较大的时间范围（过去和将来）进行预测。
尽管有嘈杂的自动注释的伪地面真相，但时间上下文也允许进行平滑的手势预测。 由于多模式，我们不希望我们的预测运动与地面真实情况相同。 但是，由于这是我们仅有的训练信号，因此我们仍然使用自动姿势检测来通过回归学习。 为了避免回归所有模式的均值，我们对预测的运动应用了对抗鉴别器[19]。 这样可以确保我们产生相对于当前说话者“真实”的运动。
手势是特质[34]，因为不同的说话者倾向于使用不同的动作风格（见图2）。 因此，为每个说话者学习个性化的手势模型至关重要。 为了解决这个问题，我们展示了一个庞大的144小时针对特定人的视频数据集，其中包含10位演讲者，我们可以公开使用这些数据。 我们特意挑选一组演讲者，以便为他们找到数小时清晰的单人演讲素材。
我们的演讲者背景各异：电视节目主持人，大学讲师和电视传播家。
他们涉及至少三种宗教，讨论了广泛的话题，从时事评论，死亡哲学，化学原理和摇滚历史到圣经和《古兰经》中的读物。