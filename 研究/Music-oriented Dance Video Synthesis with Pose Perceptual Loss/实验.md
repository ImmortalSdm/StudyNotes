## 数据集

## 实验步骤

所有型号均在Nvidia GeForce GTX 1080 Ti GPU上进行了培训。 在我们框架的第一阶段，该模型是在PyTorch [28]中实现的，大约需要一天的时间来训练400个周期。 对于超参数，我们设置V = 18，T = 50，t = 5，K = 16，S = 16000。
对于自我关注机制，我们设置k = 256，l = 40。
对于损失函数，将超参数{λl}设置为[20，5，1，1，1，1，1，1，1，1]，并且wGP = 1，wP = 1，wFM = 1，wL1 = 200。 尽管L1距离损耗的权重相对较大，但L1损耗的绝对值很小。
我们对所有网络使用Adam [17]，生成器的学习率为0.003，局部时间鉴别器的学习率为0.003，全局内容鉴别器的学习率为0.005。
对于将姿势转换为视频的第二阶段，该模型需要大约三天的时间进行训练，其超参数采用与[7]相同的参数。 对于ST-GCN和CRNN的训练前过程，我们还使用Adam [17]为他们提供了0.002的学习率。  ST-GCN在Let's Dance数据集上实现了46％的精度。 在FMA上对CRNN进行了预训练，前2名的准确性为67.82％。

## 评估

我们将评估以下基准和我们的模型。
- L1，在这种情况下，我们仅使用L1距离来引导发电机。
- 全局D，基于L1，我们添加了全局内容区分符。
- 本地D，基于全局D，我们添加了本地时间鉴别符。
- 我们的模型，基于局部D，我们添加姿势感知损失。 这些条件在表2中使用。

#### 我们的研究

为了评估生成的骨架序列的质量（我们的主要贡献），我们进行了一项用户研究，比较了合成骨架序列和真实的骨架序列。 我们随机采样10对不同长度的序列，并将其绘制成视频。
为了使这项研究公平，我们验证了地面真相骨架并重新标注了嘈杂的骨架。 在用户研究中，每个参与者都以随机顺序观看合成骨架序列的视频和地面真实骨架序列的视频。 然后，参与者需要选择以下两个选项之一：1）第一个视频效果更好。  2）第二个视频更好。 如图7所示，在43.0％的比较中，参与者投票赞成我们合成的骨架序列。 这项用户研究表明，我们的模型可以与真正的艺术家在相似的水平上编排图形。

#### 跨模态平菇

#### 质量评估

