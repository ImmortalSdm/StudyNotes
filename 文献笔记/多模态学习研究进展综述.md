# 多模态学习研究进展综述

[原文链接](https://zhuanlan.zhihu.com/p/39878607?utm_source=qq&utm_medium=social&utm_oi=1167206980455383040)

---

多模态学习主要包括以下几个研究方向：

- 多模态表示学习：主要研究如何将多个模态数据所蕴含的语义信息数值化为实值向量。

- 模态间映射：主要研究如何将某一特定模态数据中的信息映射至另一模态。

- 对齐：主要研究如何识别不同模态之间的部件、元素的对应关系。

- 融合：主要研究如何整合不同模态间的模型与特征。

- 协同学习：主要研究如何将信息富集的模态上学习的知识迁移到信息匮乏的模态，使各个模态的学习互相辅助。典型的方法包括多模态的零样本学习、领域自适应等

---
### 多模态表示学习

&emsp;&emsp;表示学习的目的是将被研究对象（结构化数据、图像、视频、语音、文本等）中所蕴含的语义信息抽象为实值向量。当多个模态共存时，我们需要同时从多个异质信息源提取被研究对象的特征。在单模态表示学习的基础上，多模态的表示学习还要考虑多个模态信息的一致性和互补性

- 在理论方面，[Learning Compact Hash Codes for Multimodal Representations Using Orthogonal Deep Structure](http://sci-hub.tw/10.1109/TMM.2015.2455415)提出了一种面向多模态表达的紧致哈希编码方法。该工作首先基于模态内和模态间的相关性约束，提出了一种新的深度学习模型生成哈希编码。在此基础上又提出了一种正交正则化方法降低哈希编码特征的冗余性，并给出了理论分析。

- 在应用方面，[Depression Detection via Harvesting Social Media: A Multimodal Dictionary Learning Solution](http://www.nextcenter.org/wp-content/uploads/2018/02/Depression-Detection-via-Harvesting-Social-Media-A-Multimodal-Dictionary-Learning-Solution.pdf)提出了一个面向情感检测的多模态字典学习方法及相关标准数据集。

- 在[Multi-modal Deep Embedding via Hierarchical Grounded Compositional Semantics](http://sci-hub.tw/10.1109/TCSVT.2016.2606648)中提出了一种基于层级复合语义的深度多模态嵌入方法。

- [Learning of Multimodal Representations With Random Walks on the Click Graph](http://sci-hub.tw/10.1109/TIP.2015.2507401)则以点击图上的随机游走为基础，提出了一种多模态表示学习方法。该方法将用户点击信息引入到多媒体信息检索中，并以图像、文本查询作为节点，以点击关系作为边构造了点击图。在此基础上，以点击图中节点关系作为约束构造了相应的多模态表达学习模型。

- [Partial Multi-Modal Sparse Coding via Adaptive Similarity Structure Regularization](http://sci-hub.tw/10.1145/2964284.2967201)考虑了由于数据稀疏性造成的部分模态数据缺失问题，提出了一种基于自适应相似结构正则化的部分多模态稀疏编码模型。

- [Effective Multi-Modal Retrieval based on Stacked Auto-Encoders](http://sci-hub.tw/10.14778/2732296.2732301)中基于模态内和模态间的相关关系，提出了一种基于栈式自编码器的高效多模态检索模型。

- [Sparse Multi-Modal Hashing](http://sci-hub.tw/10.1109/TMM.2013.2291214)提出了一种稀疏的多模态哈希编码方法。该方法首先利用超图建模模态间和模态内的相关关系，随后采用超图拉普拉斯稀疏编码方法同时学习多个模态的字典。

- [Spectral Multimodal Hashing and Its Application to Multimedia Retrieval]()基于对不同模态的相关性矩阵的谱分析，提出了一种谱哈希编码方法并将其应用于跨模态检索问题中，实现了基于哈希编码的快速跨模态检索。

- [Learning Discriminative Binary Codes for Large-scale Cross-modal Retrieval]()提出了一种面向大规模跨模态检索的判别性二值编码学习方法。该工作中提出的跨模态哈希编码方法直接保留了求解二值编码时的离散约束。特别地，该方法可通过学习模态特异的哈希函数得到统一的二值编码，并可将所得到的二值编码作为判别性特征用于后续分类。西安电子科技大学高新波教授团队在多模态表示学习领域也取得了丰硕的成果。

- [Pairwise Relationship Guided Deep Hashing for Cross-Modal Retrieval]()提出了一种基于成对关系导向的端对端深度哈希编码方法，并将其应用于跨模态检索问题中。

- [Hierarchical Multimodal LSTM for Dense Visual-Semantic Embedding]()中提出了一种基于层次化多模态LSTM的密集视觉-语义嵌入方法。具体而言，该工作首先提出了一种层级化的递归神经网络，该网络可以建模句子与词以及图像与图像中局部区域的层次化关系，然后利用该网络学习词、句子、图像以及图像区域的特征。

- [Multi-Modal Knowledge Representation Learning via Webly-Supervised Relationships Mining]()中通过从互联网获取有监督数据，提出了一种多模态知识表示学习方法。该方法具有以下优点：
 - 可自动从互联网平台获得可表征文本-视觉两模态关联关系的相关数据，并借此挖掘多模态数据中潜在的知识；
 - 可构造模态无关、任务无关的公共知识表示空间；
 - 通过迁移从已知节点和关系中所学的知识，能够表示在已知样本上未观测到的多模态关系。

- [Learning Consistent Feature Representation for Cross-Modal Multimedia Retrieval]()中提出了一种面向跨模态检索的一致表示学习方法。特别地，该方法可同时学习多个模态的基矩阵。另外，该方法还采用了局部组稀疏的正则项以保证多模态特征的一致性。

未来研究展望：

&emsp;&emsp;从本文综述的相关研究成果来看，按多模态表示共享的方式可将多模态表示学习分为两类：一类方法将所有模态的特征均投影到同一个表示空间，我们称此类方法为公共表示学习；另一类方法则为不同模态学习不同的特征表示空间，我们称此类方法为特异性表示学习。公共表示学习方法适用于所有模态数据在测试阶段都可使用的情况。相对而言，特异性表示学习由于分别学习不同模态的特征，更加适合测试阶段仅提供单模态数据或部分模态数据可用的情况，例如零次学习、模态间映射、跨模态检索等任务。对于特异性表示学习，相关工作往往仅限于两个模态的情况，对于更多模态同时存在的情况下的特异性表示学习则有待进一步研究。此外，表示学习的主流方法往往只局限于静态条件下，而使用多模态数据，如何进行动态学习是一个很有价值的研究点。

---
### 模态间映射

&emsp;&emsp;多模态机器学习中另一个重要的问题是发现知识在不同模态间的映射关系。给定实体在一种模态下的表示，模态间映射是将该表示转换成其它模态下表示的过程。例如，给定一幅图像，我们希望得到一个描述该图像的句子，或者给出一段文字描述来生成一幅与之相匹配的图像。模态间映射一直是学术界研究的热点问题，早期工作包括语音合成、图像视频描述以及跨模态检索等。最近，学术界对模态间映射的主要兴趣集中在如何将计算机视觉和自然语言处理领域最新的研究成果结合起来，并应用在大规模数据库上得到合理的视觉场景描述，其中微软发布的COCO是目前图像视频标注任务公认的数据集。


- [Video Captioning With Attention-Based LSTM and Semantic Consistency]()提出一种基于注意力机制的LSTM来完成视频标注，该方法利用语义一致性，能够捕捉视频的显著结构，探索多模态表示之间的关系，以生成具有丰富语义内容的句子来描述视频内容；

- [Hierarchical LSTM with Adjusted Temporal Attention for Video Captioning]()提出一种可以调整时序注意力的层次LSTM结构，利用时间注意力选择特定的帧来预测相关词，而调整后的时间注意力可以用于决定相关词是否依赖于视觉信息或语言上下文信息；

- [Adaptively Attending to Visual Attributes and Linguistic Knowledge for Captioning]()提出一种能够基于语言知识选择性地关注视觉属性的标注方法，该方法将神经网络中的隐藏状态映射到潜在嵌入空间，并采用注意机制获得语言知识表示，从而获得语言与视觉属性的对应关系。

- [Boosting Image Captioning with Attributes]()提出一种包含属性的LSTM和RNN网络来发现图像视觉属性与语义表达之间的复杂关系

- [Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects]()提出一种基于拷贝机制的图像标注方法，该方法通过将检测到的物体与拷贝机制相结合来预测图像标题中的新物体；

- 他们还关注了如何从句子获得对应视频的问题，提出时序生成对抗网络[To Create What You Tell: Generating Videos from Captions]()，利用语义标注作为条件来建模视频中物体的时空关系；

- 提出一种具有多模态注意力机制的LSTM[Learning Multimodal Attention LSTM Networks for Video Captioning]()，该方法设计了一个多层次的注意力机制来获取关于时间序列和多模态流数据中的关键线索。

- 提出了基于区域注意力机制和场景特异上下文信息的图像标注方法[Aligning Where to See and What to Tell: Image Captioning with Region-Based Attention and Scene-Specific Contexts]()，该方法能够协调生成描述和注意力在视觉区域之间的转移，同时将场景特定的上下文引入到LSTM中，获得特定场景类型下的语言模型用于词汇生成。

- 提出一种能够获得更具有多样性的图像标注方法[Diverse Image Captioning via GroupTalk]()，该方法能够同时学习多种语言描述的分布，同时模仿人类撰写图像标注的多样性。

- 提出了一种弱监督视频标注方法[Weakly Supervised Dense Video Captioning]()，该方法能够在训练过程中为视频剪辑生成多个不同的视频标注，所使用的监督信息仅仅是视频级别的描述语句。

- 提出了名为“Image2Text”的多模态标注生成算法，该算法将输入图像表达为被检测物体的序列输入卷积神经网络中并获得最终的图像标注。

- 提出基于空间和通道注意力机制的图像标注算法[SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning]()，该方法能够在卷积神经网络中对生成标注的上下文进行动态建模，从而确定注意力机制关注的位置和对象。

- 提出一种利用物体和场景信息的图像标注方法，该方法同时关注了图片中的物体信息和场景信息，从而获得更好的标注结果。

- 提出基于训练样本作为参考的LSTM模型，能够有效解决在图像标注问题中词汇重要性被错误对待及物体或场景被错误识别的问题。

未来研究展望：

&emsp;&emsp;多模态映射问题面临的一大问题是难以设计评价指标来度量模型的优劣。尤其是在某些生成式的任务中，如对图像进行描述和标注，往往不存在唯一正确的“标准答案”，映射过程容易受到主观影响，使得最终结果无法确认同一实体在不同模态间的表示。尽管我们也可以通过人工评分或两两比较来评价模型的映射质量以获得最接近人类认知的质量评价，但这类人工方式往往颇为耗时，且成本较高，标注结果受测试者性别、年龄、文化背景等偏差的影响而导致评价失准。因此，学界提出了一系列相关的自动评价指标，如BLEU、ROUGE、Meteor、CIDEr等。但相关研究指出这类自动指标尚不能很好地刻画映射结果的主观性。综上所述，解决映射过程中的主观评价问题不仅可以更好地评价不同方法，而且可以辅助设计更好的优化目标函数，从而全面提升模型性能。

---
### 对齐

&emsp;&emsp;对齐旨在挖掘不同模态之间子部件的相关或对应关系，从而促使学习到的多模态表示更加精确，并且也为多媒体检索提供更细致的检索线索。

- 提出了一种利用最大边距学习方式结合局部对齐（即视觉对象和词汇对齐）和全局对齐（即图片和语句对齐）方法来学习共同嵌入表示空间[Deep Compositional Cross-modal Learning to Rank via Local-Global Alignment]()，对齐后的跨语义表示可以较好地提高跨模态检索的质量。

- 提出了一种基于判别性字典学习的跨模态检索方法[Discriminative Dictionary Learning With Common Label Alignment for Cross-Modal Retrieval]()，该方法学习判别性字典来解释每种模态，不仅增强了来自不同类别的模态内数据的辨别能力，而且增强了同一类中的模态间数据的相关性，而后将编码映射到特征空间，通过标签对齐方法进一步增强跨模态数据的区分性和相关性。

- 提出了一种深层跨模态对齐网络[Deep Cross-Modality Alignment for Multi-Shot Person Re-IDentification]()，联合行人序列与图像数据来训练得到多次行人重识别模型，网络中将行人图像映射到序列数据空间并进行对齐，从而尽可能消除模态间的不匹配问题。

未来研究展望：

&emsp;&emsp;早期的多模态对齐主要依靠基于概率图模型、动态规划等无监督学习方法进行不同模态间的元素匹配。近年来，虽然已陆续有学者进行有监督的对齐方法研究，但现阶段的对齐方法仍然存在以下几点主要问题有待进一步研究：
1. 显式标注对齐信息的数据较少，不利于进行实验分析。
2. 设计不同模态之间的相似度度量指标较为困难，且人工设计费时费力。
3. 不同模态间元素的对齐过程往往存在一对多的关系，甚至还可能存在无法匹配的情况。
4. 受噪声影响大，尤其是当元素的匹配错位时模型性能下降严重。

目前，随着度量学习的发展，直接采用有监督学习方法确定有效的模态间相似度度量已成为可能。在未来的工作中，研究者可以通过设计同时进行度量学习和对齐的方法提高相关模型的性能。

---
### 融合

&emsp;&emsp;多模态融合旨在将多个模态信息整合以得到一致、公共的模型输出，是多模态领域的一个基本问题。多模态信息的融合能获得更全面的特征，提高模型鲁棒性，并且保证模型在某些模态缺失时仍能有效工作。

- 设计了一种隐含条件随机场[The classification of multi-modal data with hidden conditional random field]()，假设不同模态的数据共享潜在的结构，通过多模态数据间的联系来学习这种潜在共享结构，同时挖掘该结构与监督类别信息间的相互作用，从而应用于分类任务。

- 提出了一种新的多模态事件主题模型来建模社交媒体文档[Multi-Modal Event Topic Model for Social Event Analysis]()，通过学习文本和视觉特征间的相关性，以区分视觉代表性主题和非视觉代表性话题，并采用增量式学习策略以帮助理解社交事件的演变趋势；

- 同时还提出了一种多模态多视角的主题-意见挖掘模型[Multi-modal Multi-view Topic-opinion Mining for Social Event Analysis]()，有效结合多模态和多视角的特征用于社交事件分析，该方法不仅能挖掘多模态数据中的共同主题，还能总结出它们在每个特定主题上的异同，并且挖掘对不同主题的多视角意见集合。

- 提出了一种新型哈希算法[Weakly Supervised Multimodal Kernel for Categorizing Aerial Photographs]()，将弱监督方式提取出的多模态特征统一整合为二进制编码，从而使用核函数配合SVM进行分类。

- 通过多层线性融合双频GPS定位与多个参考接收天线运动信息来检测系统的定位误差[Multimodal data fusion for SB-JPALS status prediction under antenna motion fault mode]()。

- 提出一种新型端到端的深度融合卷积神经网络[Multimodal 2D+3D Facial Expression Recognition With Deep Fusion Convolutional Neural Network]()，将二维与三维数据输入网络进行特征提取和融合，进而获得高度集中的特征表示，进行人脸表情识别。

- 合作提出了一种带注意力机制的递归神经网络[Multimodal Fusion with Recurrent Neural Networks for Rumor Detection on Microblogs]()，利用LSTM网络融合文本和社交上下文特征，再利用注意力机制将其与图像特征融合，进行端到端的谣言预测。

未来研究展望：

&emsp;&emsp;近年来，多模态融合问题被国内外学者广泛关注，已经陆续提出基于模型无关、图模型、神经网络的多种多模态融合方法。尽管学术界在多模态融合领域已经取得了诸多进展，但现阶段的研究仍存在一些问题。每一种模态会受到不同类型和不同程度的噪声影响，导致融合得到的信息不能准确表达出应有的特征，并且在包含时序关系的多模态学习（如一段有声视频）中，每种模态可能遭受噪声干扰的时刻也可能不同。此外，模态与模态之间在时序上没有对齐，如视频的音画不同步，也可能对多模态的融合造成较大的影响。

---
### 协同学习

&emsp;&emsp;在缺乏标注数据、样本存在大量噪声以及数据收集质量不可靠时，可通过不同模态间的知识迁移提高质量较差模态的性能。

- 提出了一种跨模态知识迁移网络将跨模态数据转换为共同表示用于检索[Cross-modal Common Representation Learning by Hybrid Transfer Network]()，其中模态共享迁移子网络利用源域和目标域的模式作为桥梁，将知识同时迁移到两种模态，而层共享相关子网络保留固有的跨模态语义相关性以进一步适应跨模式检索任务。

未来研究展望：

&emsp;&emsp;由于不同模态所包含的信息不尽相同，多模态协同学习主要利用从一种模态中学到的信息来补充完善另一种模态数据的训练。其中协同训练、零次学习等问题在视觉分类、音声识别等方面得到广泛的应用。同时，协同学习方法是与需要解决的任务无关的，因此它可以用于辅助多模态映射、融合及对齐等问题的研究。基于协同学习本身的特点，如何挖掘得到尽可能多的模态间的不同信息来促进模型的学习是一个很有价值的研究方向。

---
