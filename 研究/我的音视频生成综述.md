## 摘要

本文对最近的音视频生成发展进行了全面的调查。

本文将音视频生成分为三个不同的方向：

音频生成图像
身体动作生成
发言者的脸部动作生成。

进一步讨论了最新方法以及每个子领域的剩余的挑战。

最后，我们总结了常用的数据集和性能指标。

---

## 1 引言


---

## 3 Talking Face Generation

- You said that?: Synthesising talking faces from audio
提出了一种编码器-解码器CNN模型，该模型使用面部和音频的联合嵌入来生成合成的会说话的面部视频帧。
- Talking Face Generation by Adversarially Disentangled Audio-Visual Representation
提出了一种分离式视听系统（DAVS）框架 通过利用Word-ID标签，学习具有“判别性语音信息”的联合视听嵌入空间Word-ID，然后通过对抗学习将Word-ID空间与person-ID空间分离。
- Hierarchical Cross-Modal Talking Face Generation with Dynamic Pixel-Wise Loss
提出了一种级联会说话的面部视频生成方法，该方法利用面部地标作为中间的高级表示来弥合两种不同模态之间的差距。 
- Animating Face using Disentangled Audio Representations
我们提出了一种新颖的分离音频表示学习框架，用于产生talking face的任务。 就我们所知，这是从音频表示学习的角度提高性能的第一个方法。
- FLNet: Landmark Driven Fetching and Learning Network for Faithful Talking Facial Animation Synthesis
我们提出了一种基于端到端的系统，该系统结合了基于外观和基于变形的方法来进行面部动画生成。
- Neural Voice Puppetry: Audio-driven Facial Reenactment
在这项工作中，我们提出了一种新颖的音频驱动的面部重现方法，该方法在不同的音频源中得到了推广。
这不仅使我们能够从另一个人的音频序列中合成出一个正在说话的头部的视频，而且还能够基于合成的声音生成逼真的视频。
- Towards Automatic Face-to-Face Translation
（1）我们首次设计和训练了一条自动管道，以解决面对面翻译的新问题。 我们的系统可以通过逼真的嘴唇同步功能自动将一个人的说话脸翻译成给定的目标语言。
（2）我们提出了一种新颖的模型LipGAN，用于生成以任何语言的音频为条件的逼真的说话人脸。 我们的模型在定量评估和基于人的评估方面均优于现有工作。
- Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose
我们提出了一种新颖的深度神经网络模型，该模型可以通过个性化的头部姿势和嘴唇同步，将任意来源人的音频信号转换为任意目标人的高质量有声面部视频。
- AUDIO-VISUAL DECISION FUSION FOR WFST-BASED AND SEQ2SEQ MODELS
我们提出了一种新颖的方法，可以在推理时融合音频和视频形式的信息。 这使我们能够独立训练声学和视觉模型。
- Capture, Learning, and Synthesis of 3D Speaking Styles 
我们介绍了VOCA，这是一种简单且通用的语音驱动的面部动画框架，可在多种身份下使用。
有疑问
- Disjoint Mapping Network for Cross-modal Matching of Voices and Faces 
我们提出了DIMNets，该框架将语音和面部的交叉模式匹配问题阐述为通过从一个或多个协变量进行单独监督来学习二者的通用嵌入，而不是尝试将语音映射到 直接面对。
- High-resolution talking face generation via mutual information approximation 
引入了互信息近似（MIA）来描述视频和语音之间的连贯性，从而改善了对抗学习中的重构和推理能力。
- ObamaNet Photo-realistic lip-sync from text 
我们提出了ObamaNet，这是第一个将任何文本作为输入并生成相应语音和同步逼真的口型同步视频的体系结构。
- Synthesizing Obama Learning Lip Sync from Audio
一个主要的贡献是我们最新的神经网络技术，用于从音频中合成嘴形，并在数百万个视频帧上进行训练，这比以前的方法明显简单得多，但却产生了令人信服的结果。
- Speech-Driven Facial Reenactment Using Conditional Generative Adversarial Networks
我们给出了一种新颖的方法，可以在给定音频输入的情况下，通过精确的唇形同步来生成面部的逼真图像
- End-to-End Speech-Driven Facial Animation with Temporal GANs
我们提出了一种临时的生成对抗网络（GAN），该网络能够从音频信号和单个静态图像生成讲话头的视频（请参见图1）。
- Lip Movements Generation at a Glance
提出了一个新的生成网络，一种新颖的视听相关损耗以及考虑了四个互补损耗的完整模型。 
- X2Face: A network for controlling face generation using images, audio, and pose codes
提出了一个自我监督框架X2Face，用于使用另一张面孔驱动面孔生成。

---


## 4 Body Motion Generation

- Audio to Body Dynamics

- Dance with Melody: An LSTM-autoencoder Approach to Music-oriented Dance Synthesis

- Learning Individual Styles of Conversational Gesture

- LISTEN TO DANCE: MUSIC-DRIVEN CHOREOGRAPHY GENERATION USING AUTOREGRESSIVE ENCODER-DECODER NETWORK

- Music-oriented Dance Video Synthesis with Pose Perceptual Loss

- Unsupervised Any-to-Many Audiovisual Synthesis via Exemplar Autoencoders

- Weakly-Supervised Deep Recurrent Neural Networks for Basic Dance Step Generation

---

## 5 Audio to Image

### 声音重建人脸

一个人的声音与他的面部结构、性别、年龄以及种族在统计学上都具有相关性[Face Reconstruction from Voice using Generative Adversarial Networks]。

因此，很多研究人员都致力于通过一个人的声音去合成他的脸部外观。

- Face Reconstruction from Voice using Generative Adversarial Networks   
提出了一个基于生成对抗网络（GAN）的简单而有效的计算框架。
- WAV2PIX: SPEECH-CONDITIONED FACE GENERATION USING GENERATIVE ADVERSARIAL NETWORKS
提出了一种条件GAN，它能够直接从原始语音信号（我们称为Wav2Pix）生成面部图像。
- Seeing Voices and Hearing Faces: Cross-modal biometric matching
介绍用于二进制和多路交叉模式人脸和音频匹配的CNN架构。
- Speech2Face: Learning the Face Behind a Voice

### 声音产生图像

- Image generation associated with music data

- deepsing: Generating Sentiment-aware Visual Stories using Cross-modal Music Translation

- Deep Cross-Modal Audio-Visual Generation

- TOWARDS AUDIO TO SCENE IMAGE SYNTHESIS USING GENERATIVE ADVERSARIAL NETWORK

- CMCGAN: A Uniform Framework for Cross-Modal Visual-Audio Mutual Generation

---





## 7 讨论

### 7.1 挑战

### 7.2 未来研究方向

---

## 8 结论


