## 激活函数

#### 阶跃函数
在结点突然变化
![img](../imgs/4193ff41-ba9d-416a-92b5-3849f0c61dc7.png)
- 只允许实数：
```
def step_function(x):
    if x > 0:
        return 1
    else:
        return 0
```
- 允许数组：
```
import numpy as np
def step_function_numpy(x):
    y = x > 0
    return y.astype(np.int)
```
---
#### sigmoid函数
s型函数，平滑过渡
![img](../imgs/5fc9f430-2451-4557-b2e5-a0b08935586f.png)
```
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

---
#### ReLU函数
![img](../imgs/dff26c70-3615-4100-9184-544251340ecd.png)
```
def relu(x):
    return np.maximum(0,x)
```
---
#### 恒等函数
![img](../imgs/3bfd79a5-5cfc-49f4-865e-4d7141f91bdf.png)
```
def Identity(x):
    return x
```
---
#### softmax函数
输出层的各个神经元都受到所有输入信号的影响。
输出值的总和是1，因此softmax函数的输出值为概率。
![img](../imgs/0e19dbdc-a69d-11e9-a2a3-2a2ae2dbcce4.png)
- 溢出
```
def softmax(x):
    exp_x=np.exp(x)
    sum_exp_x=np.sum(exp_x)
    return exp_x/sum_exp_x
```

- 不溢出
```
def softmax_noSpillover(x):
    x_max=np.max(x)
    exp_x=np.exp(x-x_max)
    sum_exp_x=np.sum(exp_x)
    return exp_x/sum_exp_x
```
