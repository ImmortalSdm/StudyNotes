## 摘要

本文对最近的音视频生成发展进行了全面的调查。

本文将音视频生成分为三个不同的方向：

音频生成图像
身体动作生成
发言者的脸部动作生成。

进一步讨论了最新方法以及每个子领域的剩余的挑战。

最后，我们总结了常用的数据集和性能指标。

---

## 1 引言


---

## 3 Talking Face Generation

- You said that?: Synthesising talking faces from audio
早期，Chung等[1]，尝试使用编码器-解码器结构的Convolutional Neural Networks（CNN）模型，直接学习原始音频与视频之间的对应关系。

- ObamaNet Photo-realistic lip-sync from text  
而Kumar等[2]，提出了ObamaNet，一种将文本作为输入并生成相应语音以及同步口型视频的体系结构。

---

- High-resolution talking face generation via mutual information approximation  
随后，Zhu等[3]，引入了互信息近似（mutual information approximation，MIA）来描述视频与语音之间的连贯性，从而改善了对抗学习中的重构和推理能力。

- Speech-Driven Facial Reenactment Using Conditional Generative Adversarial Networks 
Jalalifar等[4]，通过结合反馈型神经网络（recurrent neural network）以及条件生成对抗网络（conditional generative adversarial networks，C-GANs），产生更加精确自然的面部视频。

- End-to-End Speech-Driven Facial Animation with Temporal GANs 
Vougioukas等[5]，提出使用时空生成对抗网络（Temporal generative adversarial networks）进行音频驱动的面部视频的端到端模型。

- X2Face: A network for controlling face generation using images, audio, and pose codes 
Wiles等[6]，提出了一个自我监督框架X2Face，使用另一个脸部来驱动脸部的生成。框架不对输入图像的姿势，表情或身份做出任何假设，因此对于那些不受约束的设置会更加鲁棒。

---


- Talking Face Generation by Adversarially Disentangled Audio-Visual Representation
近来，Zhou等[7]，提出了一种分离式的音频-视觉系统（Disentangled Audio-Visual System，DAVS），通过对抗性学习来分离人的身份和言语信息，以更好地生成发言的面部视频。

- Hierarchical Cross-Modal Talking Face Generation with Dynamic Pixel-Wise Loss
Chen等[8]，使用一种级联GAN方法，避免了与语音内容无关的视听信号之间的虚假相关性。 还提出一种注意力机制的动态像素损失，避免像素抖动问题并使网络专注于与视听相关的区域。

- Animating Face using Disentangled Audio Representations
Mittal[9]等，提出了一种分离音频表示学习框架，这是从音频表示学习的角度提高生成发言面部视频性能的方法。

[9] Gaurav Mittal, Baoyuan Wang. Animating Face using Disentangled Audio Representations. WACV,2020.

- FLNet: Landmark Driven Fetching and Learning Network for Faithful Talking Facial Animation Synthesis
Gu[10]等，提出了一种双流网络，从多个源图像（而不是单个图像）生成更多的面部细节。



- Neural Voice Puppetry: Audio-driven Facial Reenactment
Thies等[11]，提出了一种通用的音频驱动的面部动画方法，可以将目标人员的视频与任何其他人员的音频进行合成发言视频。

- Towards Automatic Face-to-Face Translation
（1）我们首次设计和训练了一条自动管道，以解决面对面翻译的新问题。 我们的系统可以通过逼真的嘴唇同步功能自动将一个人的说话脸翻译成给定的目标语言。
（2）我们提出了一种新颖的模型LipGAN，用于生成以任何语言的音频为条件的逼真的说话人脸。 我们的模型在定量评估和基于人的评估方面均优于现有工作。
- Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose
我们提出了一种新颖的深度神经网络模型，该模型可以通过个性化的头部姿势和嘴唇同步，将任意来源人的音频信号转换为任意目标人的高质量有声面部视频。
- AUDIO-VISUAL DECISION FUSION FOR WFST-BASED AND SEQ2SEQ MODELS
我们提出了一种新颖的方法，可以在推理时融合音频和视频形式的信息。 这使我们能够独立训练声学和视觉模型。
- Capture, Learning, and Synthesis of 3D Speaking Styles 
我们介绍了VOCA，这是一种简单且通用的语音驱动的面部动画框架，可在多种身份下使用。
有疑问
- Disjoint Mapping Network for Cross-modal Matching of Voices and Faces 
我们提出了DIMNets，该框架将语音和面部的交叉模式匹配问题阐述为通过从一个或多个协变量进行单独监督来学习二者的通用嵌入，而不是尝试将语音映射到 直接面对。





---


## 4 Body Motion Generation

- Audio to Body Dynamics

- Dance with Melody: An LSTM-autoencoder Approach to Music-oriented Dance Synthesis

- Learning Individual Styles of Conversational Gesture

- LISTEN TO DANCE: MUSIC-DRIVEN CHOREOGRAPHY GENERATION USING AUTOREGRESSIVE ENCODER-DECODER NETWORK

- Music-oriented Dance Video Synthesis with Pose Perceptual Loss

- Unsupervised Any-to-Many Audiovisual Synthesis via Exemplar Autoencoders

- Weakly-Supervised Deep Recurrent Neural Networks for Basic Dance Step Generation

---

## 5 Audio to Image

### 声音重建人脸

一个人的声音与他的面部结构、性别、年龄以及种族在统计学上都具有相关性[Face Reconstruction from Voice using Generative Adversarial Networks]。

因此，很多研究人员都致力于通过一个人的声音去合成他的脸部外观。

- Face Reconstruction from Voice using Generative Adversarial Networks   
提出了一个基于生成对抗网络（GAN）的简单而有效的计算框架。
- WAV2PIX: SPEECH-CONDITIONED FACE GENERATION USING GENERATIVE ADVERSARIAL NETWORKS
提出了一种条件GAN，它能够直接从原始语音信号（我们称为Wav2Pix）生成面部图像。
- Seeing Voices and Hearing Faces: Cross-modal biometric matching
介绍用于二进制和多路交叉模式人脸和音频匹配的CNN架构。
- Speech2Face: Learning the Face Behind a Voice

### 声音产生图像

- Image generation associated with music data

- deepsing: Generating Sentiment-aware Visual Stories using Cross-modal Music Translation

- Deep Cross-Modal Audio-Visual Generation

- TOWARDS AUDIO TO SCENE IMAGE SYNTHESIS USING GENERATIVE ADVERSARIAL NETWORK

- CMCGAN: A Uniform Framework for Cross-Modal Visual-Audio Mutual Generation

---





## 7 讨论

### 7.1 挑战

### 7.2 未来研究方向

---

## 8 结论




---

[1] J. S. Chung, A. Jamaludin, and A. Zisserman, “You said that?” CoRR, 2017.

[2] R. Kumar, J. Sotelo, K. Kumar, A. de Brebisson, and ´
Y. Bengio, “Obamanet: Photo-realistic lip-sync from text,”
arXiv preprint arXiv:1801.01442, 2017.

[3] H. Zhu, A. Zheng, H. Huang, and R. He, “High-resolution talking face generation via mutual information approximation,” arXiv preprint arXiv:1812.06589, 2018.

[4] S. A. Jalalifar, H. Hasani, and H. Aghajan, “Speech-driven facial reenactment using conditional generative adversarial networks,” CoRR, 2018.

[5] K. Vougioukas, S. Petridis, and M. Pantic, “End-to-end speech-driven facial animation with temporal gans,” in BMVC, 2018.

[6] O. Wiles, A. Koepke, and A. Zisserman, “X2face: A network for controlling face generation by using images, audio, and pose codes,” in European Conference on Computer Vision, 2018.

[7] H. Zhou, Y. Liu, Z. Liu, P. Luo, and X. Wang, “Talking face generation by adversarially disentangled audio-visual representation,” CoRR, 2018.

[8] Z. D. C. X. Lele Chen, Ross K Maddox, “Hierarchical cross-modal talking face generation with dynamic pixelwise loss,” in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.

[9] Gaurav Mittal, Baoyuan Wang. Animating Face using Disentangled Audio Representations. WACV, 2020.

[10] Kuangxiao Gu, Yuqian Zhou, Thomas Huang. FLNet: Landmark Driven Fetching and Learning Network for Faithful Talking Facial Animation Synthesis. AAAI, 2020.

[11] Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, Matthias Nießner. Neural Voice Puppetry: Audio-driven Facial Reenactment. 

---