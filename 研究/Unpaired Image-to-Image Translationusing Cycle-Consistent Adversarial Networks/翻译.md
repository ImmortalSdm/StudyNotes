## 摘要

Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs.<br/>
图像到图像的翻译是一类视觉和图形问题，其目标是使用对齐的图像对训练集来学习输入图像和输出图像之间的映射。

However, for many tasks,paired training data will not be available. <br/>
但是，对于许多任务，配对的训练数据将不可用。

We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples.<br/> 
我们提出了一种在没有配对示例的情况下学习将图像从源域X转换为目标域Y的方法。

Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss.<br/>
我们的目标是学习一个映射G：X→Y，使得来自G（X）的图像分布与D的分布使用对抗损失是无法区分的。

Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F (G(X)) ≈ X (and vice versa). <br/>
由于此映射的约束严重不足，因此我们将其与反映射F：Y→X耦合，并引入循环一致性损失以强制执行F（G（X））≈X（反之亦然）。

Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer,photo enhancement, etc. <br/>
定性结果在不存在配对训练数据的多个任务上显示，包括收集样式转移，对象变形，季节转移，照片增强等。

Quantitative comparisons against several prior methods demonstrate the superiority of our approach.<br/>
与几种先前方法的定量比较证明了我们方法的优越性。

---

## 引言

What did Claude Monet see as he placed his easel by the bank of the Seine near Argenteuil on a lovely spring day in 1873 (Figure 1, top-left)? <br/>
克劳德·莫奈（Claude Monet）在1873年一个可爱的春天里，将画架放在塞纳河岸附近的阿根廷人附近时，看到了什么（图1，左上）？

A color photograph, had it been invented, may have documented a crisp blue sky and a glassy river reflecting it. <br/>
如果发明了彩色照片，则可能记录了湛蓝的天空和反射着它的玻璃状河流。

Monet conveyed his impression of this same scene through wispy brush strokes and a bright palette.<br/>
莫奈通过细腻的笔触和明亮的调色板传达了他对同一场景的印象。

What if Monet had happened upon the little harbor in Cassis on a cool summer evening (Figure 1, bottom-left)?<br/>
如果莫奈在一个凉爽的夏日夜晚发生在卡西斯（Cassis）的小港口上，该怎么办（图1，左下）？

A brief stroll through a gallery of Monet paintings makes it possible to imagine how he would have rendered the scene: perhaps in pastel shades, with abrupt dabs of paint, and a somewhat flattened dynamic range.<br/>
短暂浏览莫奈画作的画廊，可以想象他将如何渲染场景：也许是柔和的阴影，突然的油漆点涂和动态范围平坦。

We can imagine all this despite never having seen a side by side example of a Monet painting next to a photo of the scene he painted. <br/>
我们可以想象所有这一切，尽管从未在他所画的场景照片旁边看到莫奈画作的并排示例。

Instead, we have knowledge of the set of Monet paintings and of the set of landscape photographs.<br/>
我们可以想象所有这一切，尽管从未在他所画的场景照片旁边看到莫奈画作的并排示例。

We can reason about the stylistic differences between these two sets, and thereby imagine what a scene might look like if we were to “translate” it from one set into the other.<br/>
取而代之的是，我们了解了莫奈的绘画和风景照。

In this paper, we present a method that can learn to do the same: capturing special characteristics of one image collection and figuring out how these characteristics could be translated into the other image collection, all in the absence of any paired training examples.<br/>
我们可以推断出这两个集合之间的风格差异，从而可以想象如果将场景从一个集合“转换”到另一个集合中，场景将是什么样子。

This problem can be more broadly described as imageto-image translation [22], converting an image from one representation of a given scene, x, to another, y, e.g.,grayscale to color, image to semantic labels, edge-map to photograph. <br/>
这个问题可以更广泛地描述为图像到图像的转换[22]，将图像从给定场景x的一种表示形式转换为另一种y，例如将灰度转换为颜色，将图像转换为语义标签，将边缘映射转换为照片。 。

Years of research in computer vision, image processing, computational photography, and graphics have produced powerful translation systems in the supervised setting, where example image pairs {x i , y i } N i=1 are available (Figure 2, left), e.g., [11, 19, 22, 23, 28, 33, 45, 56, 58,62]. <br/>
多年来在计算机视觉，图像处理，计算摄影和图形方面的研究已经在有监督的环境下产生了功能强大的翻译系统，其中示例图像对{xi，yi} N i = 1可用（图2，左），例如[ 11、19、22、23、28、33、45、56、58、62]。

However, obtaining paired training data can be difficult and expensive. For example, only a couple of datasets exist for tasks like semantic segmentation (e.g., [4]), and they are relatively small.<br/>
然而，获得成对的训练数据可能是困难且昂贵的。例如，仅存在用于语义分割等任务的几个数据集（例如[4]），并且它们相对较小。

Obtaining input-output pairs for graphics tasks like artistic stylization can be even more difficult since the desired output is highly complex, typically requiring artistic authoring. <br/>
由于所需的输出高度复杂（通常需要艺术创作），因此获取图形任务（如艺术风格）的输入输出对可能会更加困难。

For many tasks, like object transfiguration (e.g., zebra↔horse, Figure 1 top-middle), the desired output is not even well-defined.<br/>
对于许多任务，例如对象变形（例如，斑马，图1居中），所需的输出甚至都没有明确定义。

We therefore seek an algorithm that can learn to translate between domains without paired input-output examples (Figure 2, right). <br/>
因此，我们寻求一种无需对输入/输出示例进行配对即可学会在域之间进行翻译的算法（右图2）。

We assume there is some underlying relationship between the domains – for example, that they are two different renderings of the same underlying scene – and seek to learn that relationship.<br/>
我们假设这些域之间存在某种潜在的关系-例如，它们是同一基础场景的两个不同渲染-并试图学习这种关系。

Although we lack supervision in the form of paired examples, we can exploit supervision at the level of sets: we are given one set of images in domain X and a different set in domain Y . <br/>
尽管我们缺乏成对示例形式的监督，但是我们可以在集合级别上利用监督：我们在域X中获得了一组图像，在域Y中获得了一组不同的图像。

We may train a mapping G : X → Y such that the output ŷ = G(x), x ∈ X, is indistinguishable from images y ∈ Y by an adversary trained to classify ŷ apart from y. <br/>
我们可以训练一个映射G：X→Y，使得输出的ŷ= G（x），x∈X与被图像分类为y的对手无法区分。

In theory, this objective can induce an output distribution over ŷ that matches the empirical distribution p data (y) (in general, this requires G to be stochastic) [16]. <br/>
从理论上讲，该目标可以在ŷ上得出与经验分布p数据（y）匹配的输出分布（通常，这要求G是随机的）[16]。

The optimal G thereby translates the domain X to a domain Ŷ distributed identically to Y .<br/>
因此，最优G将域X转换为与Y分布相同的域Ŷ。

However, such a translation does not guarantee that an individual input x and output y are paired up in a meaningful way – there are infinitely many mappings G that will induce the same distribution over ŷ. <br/>
但是，这种转换并不能保证单个输入x和输出y以有意义的方式配对-存在无限多个映射G，它们将在induce上引起相同的分布。

Moreover, in practice, we have found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the wellknown problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress [15].<br/>
此外，在实践中，我们发现很难单独优化对抗目标：标准程序通常会导致众所周知的模式崩溃问题，即所有输入图像都映射到同一输出图像，并且优化无法取得进展[15] 。

These issues call for adding more structure to our objective. <br/>
这些问题要求为我们的目标增加更多的结构。

Therefore, we exploit the property that translation should be “cycle consistent”, in the sense that if we translate, e.g., a sentence from English to French, and then translate it back from French to English, we should arrive back at the original sentence [3]. <br/>
因此，我们利用翻译应该是“周期一致”的特性，即如果我们将一个句子从英语翻译成法语，然后再从法语翻译成英语，那么我们应该回到原始句子[3]。

Mathematically, if we have a translator G : X → Y and another translator F : Y → X, then G and F should be inverses of each other, and both mappings should be bijections. <br/>
从数学上讲，如果我们有一个翻译器G：X→Y和另一个翻译器F：Y→X，则G和F应该彼此相反，并且两个映射都应该是双射。

We apply this structural assumption by training both the mapping G and F simultaneously, and adding a cycle consistency loss [64] that encourages F (G(x)) ≈ x and G(F (y)) ≈ y. <br/>
我们通过同时训练映射G和F并添加鼓励F（G（x））≈x和G（F（y））≈y的循环一致性损失[64]来应用此结构假设。

Combining this loss with adversarial losses on domains X and Y yields our full objective for unpaired image-to-image translation.<br/>
将这种损失与域X和Y上的对抗性损失结合起来，就可以得出我们未成对图像到图像转换的全部目标。

We apply our method to a wide range of applications,including collection style transfer, object transfiguration,season transfer and photo enhancement. <br/>
我们将我们的方法应用到广泛的应用程序中，包括收集样式转换，对象变形，季节转换和照片增强。

We also compare against previous approaches that rely either on hand-defined factorizations of style and content, or on shared embedding functions, and show that our method outperforms these baselines. <br/>
我们还与以前的方法进行了比较，后者依靠手工定义的样式和内容的因式分解或共享的嵌入函数，并表明我们的方法优于这些基准。

We provide both PyTorch and Torch implementations.<br/>
我们提供PyTorch和Torch实施。

 Check out more results at our website.<br/>
 在我们的网站上查看更多结果。

















---