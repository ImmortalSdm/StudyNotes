## 相关工作

### 对话式手势

　　McNeill [34]将手势分为几类[34]：标志具有特定的常规含义（例如，“竖起大拇指！”）；标志传达了物理形状或运动方向；隐喻用具体动作描述抽象内容；忠实的人指的是手势，节拍是重复的快速手势，可为语音提供时间框架。
许多心理学家已经研究了与cospeech手势有关的问题[34，23]（有关评论，请参见[46]）。这项庞大的研究工作主要依靠在实验室环境中使用记录的编排的故事重述来研究少数个体对象。这些研究中的分析是手动过程。相反，我们的目标是使用数据驱动的方法在野外研究会话手势。
由于语音和手势可能不同步，因此对语音进行手势预测的条件可以说是一项模糊的任务。麦克尼尔[34]建议手势和言语源自共同的来源，因此应根据明确定义的规则及时发生，肯登[23]建议手势应在相应的发声之前开始。其他人甚至认为，语音和手势之间的时间关系尚不清楚，并且手势可以在发声之前，之后或之中出现[4]。

### 手语和象征性手势识别

　　已经进行了大量旨在识别视频中手势语手势的计算机视觉工作。
这包括使用视频成绩单作为监督的薄弱来源的方法[3]，以及基于CNN [37，26]和RNN [13]的最新方法。 也有一些工作可以识别象征性的手势和面部手势[17、14]，头部手势[35]和同声手势[38]。 相比之下，我们的目标是从音频中预测共语音手势。

### 对话代理

　　研究人员提出了许多方法来生成合理的手势，特别是对于带有对话代理的应用[8]。在早期的工作中，Cassell等人。 [7]提出了一种基于手动定义的规则来指导手臂/手部动作的系统。随后的基于规则的系统[27]提出了通过注释表达手势的新方法。
与我们的方法更紧密相关的方法是从语音和文本中学习手势的方法，而无需作者手动指定规则。值得注意的是，[9]使用口语文本的自然语言处理来合成手势，而Neff [36]提出了一种用于制作特定于人的手势的系统。莱文等。 [30]学会了使用HMM将声学韵律特征映射到运动。后来的工作[29]将这种方法扩展为使用强化学习和语音识别，将声学分析与文本结合[33]，创建了基于混合规则的系统[40]，并使用受限的Boltzmann机器进行推理[11]。由于这些方法的目标是为虚拟代理生成动作，因此它们使用实验室录制的音频，文本和动作捕获。这使他们能够使用简化的假设，对像我们这样的野外视频分析提出挑战：例如，[30]需要精确的3D姿势，并假设运动发生在音节边界上，[11]则假设手势是由手腕向上运动。与这些方法相比，我们的方法在训练过程中没有明确使用任何文本或语言信息-它从原始的视听对应关系中学习手势-也没有使用手定义的手势类别：手臂/手的姿势是直接从音频中预测的。

### 可视化预测手势

　　可视化手势的最常见方法之一是使用它们来为3D化身动画[45、29、20]。 由于我们的研究工作是针对无法获得3D数据的野生视频的个性化手势，因此我们使用了Bregler等人启发的数据驱动的综合方法。 [2]。 为此，我们使用Chan等人的“从视频到视频的姿势”方法。 [10]，它使用条件生成对抗网络（GAN）从姿势合成人体视频。

### 声音和视觉
　　Aytar等。 [1]使用自然现象中视觉和音频信号的同步来从未标记的野生视频中学习声音表示。
为此，他们将知识从视觉领域中经过训练的判别模型转移到音频领域。
音频和视频功能的同步也可以用于合成。 Langlois等。 [28]尝试通过生成物体坠落或跌落的刚体动画来优化同步事件，这些动画在时间上将所需接触事件序列的输入声波与地平面匹配。最近，Shlizerman等人。 [42]根据输入的音乐对3D头像的手进行了动画处理。但是，他们的重点是音乐演奏，而不是手势，因此，可能的运动空间有限（例如，小提琴弓的曲折运动）。
此外，虽然音乐是由产生音乐的动作（与音乐同步）唯一定义的，但手势既不是语音所独有的，也不是与语音发声同步的。
在给定音频输入的情况下，一些作品专注于合成面部表情视频的特定任务。
Chung等。 [12]通过学习面部和音频的联合嵌入，从说话者的静止图像和输入的语音片段中生成说话的面部图像。同样，[44]通过使用递归神经网络将语音音频映射到嘴巴形状，然后将合成的嘴唇嵌入地面真人面部视频中，来合成奥巴马说新颖单词的视频。虽然这两种方法都可以通过生成说不同人的话的面孔来创建虚假内容，但我们专注于针对模拟同一说话者话语而优化的单人模型。最重要的是，由于手势与语音是异步的，多模式的且针对特定的人，因此从语音生成手势而不是嘴唇动作更为复杂。