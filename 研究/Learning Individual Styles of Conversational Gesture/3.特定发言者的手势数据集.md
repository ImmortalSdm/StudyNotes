# 3.特定发言者的手势数据集
　　我们引入了一个长达144小时的大型视频数据集，专门用于以数据驱动的方式研究单个发言者的语音和手势。如图2所示，我们的数据集包含最初录制用于电视节目或大学演讲的10位手势发言人的实时视频。我们为每个演讲者收集几个小时的视频，以便我们可以分别为每个人建模。我们选择的演讲者涵盖了广泛的主题和手势风格。我们的数据集包含：5位脱口秀主持人，3位讲师和2位电话传教士。在补充材料中可以找到有关数据收集和处理以及对各种手势样式的分析的详细信息。

### 手势表示和注释
　　我们使用2D骨骼关键点的时间堆栈来表示说话者随时间的姿势，这些姿势是使用OpenPose [5]获得的。

　　从OpenPose检测到的完整关键点集中，我们使用对应于脖子，肩膀，肘部，手腕和手的49个点来表示手势。

　　连同视频素材一起，我们以15fps的速率为数据的每一帧提供骨骼关键点。

　　但是请注意，这些不是地面真相注释，而是来自最新姿态检测系统的地面真相的代理。

### 数据集注释的质量
　　所有基于真实的，都具有相关的错误。无论是来自人类观察员还是来自其他观察者。

　　我们使用自动姿态检测收集的伪基于真实可能比人类标注具有更大的误差，但它使我们能够训练大量的数据。

　　但是，我们必须估计伪基于真实的准确性是否足以支持我们的定量结论。

　　我们将自动姿势检测与在训练数据的子集上从人类观察者获得的标签进行比较，发现伪基于真相接近人类标签，并且伪基于真相的误差对于我们的任务而言足够小。

　　我们的补充材料中详细介绍了完整的实验。
